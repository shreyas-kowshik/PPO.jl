{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: activating new environment at /media/shreyas/Data/GSoC/TRPO.jl/Project.toml.\n",
      "└ @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Pkg/src/API.jl:519\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: gaussian_policy not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: gaussian_policy not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[43]:186"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a buffer object to store experiences.\n",
    "Log mean and standard deviation to buffer.\n",
    "\"\"\"\n",
    "\n",
    "using Pkg\n",
    "Pkg.activate(\"./Project.toml\")\n",
    "\n",
    "using Flux\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using Random\n",
    "using BSON:@save,@load\n",
    "using JLD\n",
    "\n",
    "include(\"policies.jl\")\n",
    "\n",
    "\"\"\"\n",
    "Utilities\n",
    "\"\"\"\n",
    "\n",
    "function get_flat_grads(gradients,models...)\n",
    "    \"\"\"\n",
    "    Flattens out the gradients and concatenates them\n",
    "    \n",
    "    Returns : Tracker Array of shape (NUM_PARAMS,1)\n",
    "    \"\"\"\n",
    "\n",
    "    flat_grads = []\n",
    "\n",
    "    function flatten!(p)\n",
    "        if typeof(p) <: TrackedArray\n",
    "            prod_size = prod(size(p))\n",
    "            push!(flat_grads,reshape(gradients[p],prod_size))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for model in models\n",
    "        mapleaves(flatten!,model)\n",
    "    end\n",
    "    \n",
    "    flat_grads = cat(flat_grads...,dims=1)\n",
    "    flat_grads = reshape(flat_grads,length(flat_grads),1)\n",
    "    \n",
    "    return flat_grads\n",
    "end\n",
    "\n",
    "function get_flat_params(models...)\n",
    "    \"\"\"\n",
    "    Flattens out the parameters and concatenates them\n",
    "    \n",
    "    Returns : Tracker Array of shape (NUM_PARAMS,1)\n",
    "    \"\"\"\n",
    "\n",
    "    flat_params = []\n",
    "    \n",
    "    function flatten!(p)\n",
    "        if typeof(p) <: TrackedArray\n",
    "            prod_size = prod(size(p))\n",
    "            push!(flat_params,reshape(p,prod_size))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for model in models\n",
    "        mapleaves(flatten!,model)\n",
    "    end\n",
    "    \n",
    "    flat_params = cat(flat_params...,dims=1)\n",
    "    flat_params = reshape(flat_params,length(flat_params),1)\n",
    "    \n",
    "    return flat_params\n",
    "end\n",
    "\n",
    "function set_flat_params(parameters,models...)\n",
    "    \"\"\"\n",
    "    Sets values of `parameters` to the `model`\n",
    "    \n",
    "    parameters : flattened out array of model parameters\n",
    "    \"\"\"\n",
    "    ptr = 1\n",
    "    \n",
    "    function assign!(p)\n",
    "        if typeof(p) <: TrackedArray\n",
    "            prod_size = prod(size(p))\n",
    "            \n",
    "            p.data .= Float32.(reshape(parameters[ptr : ptr + prod_size - 1,:],size(p)...)).data\n",
    "            ptr += prod_size\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for model in models\n",
    "        mapleaves(assign!,model)\n",
    "    end\n",
    "    \n",
    "    print(\"\")\n",
    "end\n",
    "\n",
    "function categorical_kl(states,old_log_probs)\n",
    "    action_probs = m(states)\n",
    "    log_probs = log.(action_probs)\n",
    "    \n",
    "    log_ratio = log_probs .- old_log_probs\n",
    "    kl_div = (exp.(old_log_probs)) .* log_ratio\n",
    "    return sum(kl_div,dims=1)\n",
    "end\n",
    "\n",
    "function gaussian_kl(μ0,logΣ0,μ1,logΣ1)\n",
    "    var0 = exp.(2 .* logΣ0)\n",
    "    var1 = exp.(2 .* logΣ1)\n",
    "    pre_sum = 0.5 .* (((μ0 .- μ1).^2 .+ var0) ./ (var1 .+ 1e-8) .- 1.0f0) .+ logΣ1 .- logΣ0\n",
    "    kl = sum(pre_sum,dims=1)\n",
    "    return kl\n",
    "end\n",
    "\n",
    "function kl_loss(states,actions,advantages,returns,old_log_probs)\n",
    "    if MODE == \"CON\"\n",
    "        # TODO\n",
    "    else\n",
    "        # TODO\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "# Environment Creation #\n",
    "env_name = \"Pendulum-v0\"\n",
    "MODE = \"CON\" # Can be either \"CON\" (Continuous) or \"CAT\" (Categorical)\n",
    "\n",
    "# Environment Variables #\n",
    "STATE_SIZE = 3\n",
    "ACTION_SIZE = 1\n",
    "EPISODE_LENGTH = 2000\n",
    "TEST_STEPS = 10000\n",
    "REWARD_SCALING = 16.2736044\n",
    "# Policy parameters #\n",
    "η = 3e-4 # Learning rate\n",
    "STD = 0.0 # Standard deviation\n",
    "HIDDEN_SIZE = 30\n",
    "# GAE parameters\n",
    "γ = 0.99\n",
    "λ = 0.95\n",
    "# Optimization parameters\n",
    "PPO_EPOCHS = 10\n",
    "NUM_EPISODES = 100000\n",
    "BATCH_SIZE = 256\n",
    "c₀ = 1.0\n",
    "c₁ = 1.0\n",
    "c₂ = 0.001\n",
    "# PPO parameters\n",
    "ϵ = 0.2\n",
    "# FREQUENCIES\n",
    "SAVE_FREQUENCY = 50\n",
    "VERBOSE_FREQUENCY = 5\n",
    "global_step = 0\n",
    "\n",
    "# Global variable to monitor losses\n",
    "reward_hist = []\n",
    "policy_l = 0.0\n",
    "entropy_l = 0.0\n",
    "value_l = 0.0\n",
    "\n",
    "#---------Scale rewards-------#\n",
    "function scale_rewards(rewards)\n",
    "    return (rewards  ./ REWARD_SCALING) .+ 2.0f0\n",
    "end\n",
    "\n",
    "function normalise(arr)\n",
    "    (arr .- mean(arr))./(sqrt(var(arr) + 1e-10))\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Define the networks\n",
    "\"\"\"\n",
    "\n",
    "if MODE == \"CON\"\n",
    "    policy_μ,policy_Σ = gaussian_policy(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE)\n",
    "    value = value_fn(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE,tanh)\n",
    "elseif MODE == \"CAT\"\n",
    "    policy = categorical_policy(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE)\n",
    "    value = value_fn(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE,relu)\n",
    "else \n",
    "    error(\"MODE can only be (CON) or (CAT)...\")\n",
    "end\n",
    "\n",
    "opt = ADAM(η)\n",
    "\n",
    "\"\"\"\n",
    "Functions to get rollouts\n",
    "\"\"\"\n",
    "\n",
    "function action(state)\n",
    "    # Acccounting for the element type\n",
    "    state = reshape(Array(state),length(state),1) \n",
    "\n",
    "    a = nothing\n",
    "    if MODE == \"CON\"\n",
    "        # Our policy outputs the parameters of a Normal distribution\n",
    "        μ = policy_μ(state)\n",
    "        μ = reshape(μ,ACTION_SIZE)\n",
    "        log_std = policy_Σ\n",
    "        \n",
    "        σ² = (exp.(log_std)).^2\n",
    "        Σ = diagm(0=>σ².data)\n",
    "        \n",
    "        dis = MvNormal(μ.data,Σ)\n",
    "        \n",
    "        a = rand(dis,ACTION_SIZE)\n",
    "    else\n",
    "        action_probs = policy(state).data\n",
    "        action_probs = reshape(action_probs,ACTION_SIZE)\n",
    "        a = sample(1:ACTION_SIZE,Weights(action_probs)) - 1\n",
    "    end\n",
    "    a\n",
    "end\n",
    "\n",
    "function run_episode(env)\n",
    "    experience = []\n",
    "    \n",
    "    s = reset!(env)\n",
    "    for i in 1:EPISODE_LENGTH\n",
    "        a = action(s)\n",
    "        # a = convert.(Float64,a)\n",
    "        \n",
    "        if MODE == \"CON\"\n",
    "            a = reshape(a,ACTION_SIZE)\n",
    "        end\n",
    "\n",
    "        r,s_ = step!(env,a)\n",
    "        push!(experience,(s,a,r,s_))\n",
    "        s = s_\n",
    "        if env.done\n",
    "           break \n",
    "        end\n",
    "    end\n",
    "    experience\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Multi-threaded parallel rollout collection\n",
    "\"\"\"\n",
    "\n",
    "num_processes = 3\n",
    "addprocs(num_processes) \n",
    "\n",
    "@everywhere function collect(env)\n",
    "    run_episode(env)\n",
    "end\n",
    "\n",
    "@everywhere function rollout()\n",
    "  env = GymEnv(env_name)\n",
    "  env.pyenv._max_episode_steps = EPISODE_LENGTH\n",
    "  return collect(env)\n",
    "end\n",
    "\n",
    "function get_rollouts()\n",
    "    g = []\n",
    "    for  w in workers()\n",
    "      push!(g, rollout())\n",
    "    end\n",
    "\n",
    "    fetch.(g)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Generalized Adavantage Estimation\n",
    "\"\"\"\n",
    "\n",
    "function gae(states,actions,rewards,next_states)\n",
    "    \"\"\"\n",
    "    Returns a Generalized Advantage Estimate for an episode\n",
    "    \"\"\"\n",
    "    Â = []\n",
    "    A = 0.0\n",
    "    for i in reverse(1:length(states))\n",
    "        if length(states) < EPISODE_LENGTH && i == length(states)\n",
    "            δ = rewards[i] - cpu.(value(states[i]).data[1])\n",
    "        else\n",
    "            δ = rewards[i] + γ*cpu.(value(next_states[i]).data[1]) - cpu.(value(states[i]).data[1])\n",
    "        end\n",
    "\n",
    "        A = δ + (γ*λ*A)\n",
    "        push!(Â,A)\n",
    "    end\n",
    "    \n",
    "    Â = reverse(Â)\n",
    "    return Â\n",
    "end\n",
    "\n",
    "function disconunted_returns(rewards)\n",
    "    r = 0.0\n",
    "    returns = []\n",
    "    for i in reverse(1:length(rewards))\n",
    "        r = rewards[i] + γ*r\n",
    "        push!(returns,r)\n",
    "    end\n",
    "    returns = reverse(returns)\n",
    "    returns\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Calculate Log Probabilities\n",
    "\"\"\"\n",
    "function log_prob_from_actions(states,actions)\n",
    "    \"\"\"\n",
    "    Returns log probabilities of the actions taken\n",
    "    \n",
    "    states,actions : episode vairbles in the form of a list\n",
    "    \"\"\"\n",
    "    log_probs = []\n",
    "    \n",
    "    for i in 1:length(states)\n",
    "        if MODE == \"CON\"\n",
    "            μ = reshape(policy_μ(states[i]),ACTION_SIZE).data\n",
    "            logΣ = policy_Σ.data |> cpu\n",
    "            push!(log_probs,normal_log_prob(μ,logΣ,actions[i]))\n",
    "        else\n",
    "            action_probs = policy(states[i])\n",
    "            prob = action_probs[actions[i],:].data\n",
    "            push!(log_probs,log.(prob))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    log_probs\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Process and extraction information from rollouts\n",
    "\"\"\"\n",
    "\n",
    "function process_rollouts(rollouts)\n",
    "    \"\"\"\n",
    "    rollouts : variable returned by calling `get_rollouts`\n",
    "    \n",
    "    Returns : \n",
    "    states, actions, rewards for minibatch processing\n",
    "    \"\"\"\n",
    "    # Process the variables\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    advantages = []\n",
    "    returns = []\n",
    "    log_probs = []\n",
    "    \n",
    "    # Logging statistics\n",
    "    episode_mean_returns = []\n",
    "    \n",
    "    for ro in rollouts\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        episode_next_states = []\n",
    "        \n",
    "        for i in 1:length(ro)\n",
    "             push!(episode_states,Array(ro[i][1]))\n",
    "             \n",
    "             if MODE == \"CON\"\n",
    "                 push!(episode_actions,ro[i][2])\n",
    "             else\n",
    "                 push!(episode_actions,ro[i][2] + 1)\n",
    "             end\n",
    "             \n",
    "             push!(episode_rewards,ro[i][3])\n",
    "             push!(episode_next_states,ro[i][4])\n",
    "        end\n",
    "        \n",
    "        episode_rewards = scale_rewards(episode_rewards)\n",
    "        episode_advantages = gae(episode_states,episode_actions,episode_rewards,episode_next_states)\n",
    "        episode_advantages = normalise(episode_advantages)\n",
    "        \n",
    "        episode_returns = disconunted_returns(episode_rewards)\n",
    "         \n",
    "        push!(episode_mean_returns,mean(episode_returns))\n",
    "         \n",
    "        push!(states,episode_states)\n",
    "        push!(actions,episode_actions)\n",
    "        push!(rewards,episode_rewards)\n",
    "        push!(advantages,episode_advantages)\n",
    "        push!(returns,episode_returns)\n",
    "        push!(log_probs,log_prob_from_actions(episode_states,episode_actions))\n",
    "    end\n",
    "    \n",
    "    states = cat(states...,dims=1)\n",
    "    actions = cat(actions...,dims=1)\n",
    "    rewards = cat(rewards...,dims=1)\n",
    "    advantages = cat(advantages...,dims=1)\n",
    "    returns = cat(returns...,dims=1)\n",
    "    log_probs = cat(log_probs...,dims=1)\n",
    "    \n",
    "    push!(reward_hist,mean(episode_mean_returns))\n",
    "    \n",
    "    if length(reward_hist) <= 100\n",
    "        println(\"RETURNS : $(mean(episode_mean_returns))\")\n",
    "    else\n",
    "        println(\"MEAN RETURNS : $(mean(reward_hist))\")\n",
    "        println(\"LAST 100 RETURNS : $(mean(reward_hist[end-100:end]))\")\n",
    "    end\n",
    "    \n",
    "    return hcat(states...),hcat(actions...),hcat(rewards...),hcat(advantages...),hcat(returns...),hcat(log_probs...)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Loss function definition\n",
    "\"\"\"\n",
    "\n",
    "function policy_loss(states,actions,advantages,returns,old_log_probs)\n",
    "    if MODE == \"CON\"\n",
    "        μ = policy_μ(states)\n",
    "        logΣ = policy_Σ \n",
    "        \n",
    "        new_log_probs = normal_log_prob(μ,logΣ,actions)\n",
    "    else\n",
    "        action_probs = policy(states) # ACTION_SIZE x BATCH_SIZE\n",
    "        actions_one_hot = zeros(ACTION_SIZE,size(action_probs)[end])\n",
    "        \n",
    "        for i in 1:size(action_probs)[end]\n",
    "            actions_one_hot[actions[:,i][1],i] = 1.0                \n",
    "        end\n",
    "        \n",
    "        new_log_probs = log.(sum((action_probs .+ 1f-5) .* actions_one_hot,dims=1))\n",
    "    end\n",
    "    \n",
    "    # Surrogate loss computations\n",
    "    ratio = exp.(new_log_probs .- old_log_probs)\n",
    "    π_loss = mean(ratio .* advantages)\n",
    "    return π_loss\n",
    "end\n",
    "\n",
    "function value_loss(states,returns)\n",
    "    value_predicted = value(states)\n",
    "    value_loss = mean((value_predicted .- returns).^2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Optimization Function\n",
    "\"\"\"\n",
    "\n",
    "function gvp(states,actions,advantages,returns,log_probs,x)\n",
    "    \"\"\"\n",
    "    Intermediate utility function, calculates Σ∇D_kl*x\n",
    "    \n",
    "    x : Variable to be estimated using conjugate gradient (Hx = g); (NUM_PARAMS,1)\n",
    "    \"\"\"\n",
    "    if MODE == \"CON\"\n",
    "        model_params = params(params(policy_μ)...,params(policy_Σ)...)\n",
    "        gs = Tracker.gradient(() -> policy_loss(states,actions,advantages,returns,log_probs),model_params;nest=true)\n",
    "        flat_grads = get_flat_grads(gs,policy_μ,policy_Σ)\n",
    "    else\n",
    "        model_params = params(policy)\n",
    "        gs = Tracker.gradient(() -> policy_loss(states,actions,advantages,returns,log_probs),model_params;nest=true)\n",
    "        flat_grads = get_flat_grads(gs,policy)\n",
    "    end\n",
    "    \n",
    "    return sum(x' * flat_grads)\n",
    "end\n",
    "\n",
    "function Hvp(states,actions,advantages,returns,log_probs,x)\n",
    "    \"\"\"\n",
    "    Computes the Hessian Vector Product\n",
    "    Hessian is that of the kl divergence between the old and the new policies wrt the policy parameters\n",
    "    \n",
    "    Returns : Hx; H = ∇²D_kl\n",
    "    \"\"\"\n",
    "    if MODE == \"CON\"\n",
    "        model_params = params(params(policy_μ)...,params(policy_Σ)...)\n",
    "        hessian = Tracker.gradient(() -> gvp(states,actions,advantages,returns,log_probs,x),model_params)\n",
    "        return get_flat_grads(hessian,policy_μ,policy_Σ)\n",
    "    else\n",
    "        model_params = params(policy)\n",
    "        hessian = Tracker.gradient(() -> gvp(states,actions,advantages,returns,log_probs,x),model_params)\n",
    "        return get_flat_grads(hessian,policy)\n",
    "    end\n",
    "end\n",
    "\n",
    "function conjugate_gradients(states,actions,advantages,returns,log_probs,Hvp,b,nsteps,err=1e-10)\n",
    "    \"\"\"\n",
    "    b : Array of shape (NUM_PARAMS,1)\n",
    "    \"\"\"\n",
    "    x = zeros(size(b))\n",
    "    \n",
    "    r = copy(b)\n",
    "    p = copy(b)\n",
    "    \n",
    "    rdotr = r' * r\n",
    "    \n",
    "    for i in 1:nsteps\n",
    "        hvp = Hvp(states,p).data # Returns array of shape (NUM_PARAMS,1)\n",
    "\n",
    "        α = rdotr ./ (p' * hvp)\n",
    "        \n",
    "        x = x .+ (α .* p)\n",
    "        r = r .- (α .* hvp)\n",
    "\n",
    "        new_rdotr = r' * r\n",
    "        β = new_rdotr ./ rdotr\n",
    "        p = r .+ (β .* p)\n",
    "        \n",
    "        rdotr = new_rdotr\n",
    "        \n",
    "        if rdotr[1] < err\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end\n",
    "\n",
    "function trpo_update(states,actions,advantages,returns,log_probs)\n",
    "    if MODE == \"CON\"\n",
    "        model_params = params(params(policy_μ)...,params(policy_Σ)...)\n",
    "        \n",
    "        # Obtain an estimate of H_inv * g\n",
    "        flat_policy_grads = get_flat_grads(Tracker.gradient(() -> \n",
    "                policy_loss(states,actions,advantages,returns,log_probs),model_params),policy_μ,polic_Σ).data\n",
    "        x = conjugate_gradients(states,Hvp,-1.0 .* flat_policy_grads,10) # H_inv * g\n",
    "\n",
    "        δ = 0.01\n",
    "        step_dir = sqrt.((2 * δ) ./ (x' * Hvp(states,actions,advantages,returns,log_probs,x))) .* x\n",
    "    else\n",
    "        model_params = params(policy)\n",
    "        \n",
    "        # Obtain an estimate of H_inv * g\n",
    "        flat_policy_grads = get_flat_grads(Tracker.gradient(() -> \n",
    "                policy_loss(states,actions,advantages,returns,log_probs),model_params),policy).data\n",
    "        x = conjugate_gradients(states,Hvp,-1.0 .* flat_policy_grads,10) # H_inv * g\n",
    "        \n",
    "        δ = 0.01\n",
    "        step_dir = sqrt.((2 * δ) ./ (x' * Hvp(states,actions,advantages,returns,log_probs,x))) .* x\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Train\n",
    "\"\"\"\n",
    "\n",
    "function train_step()    \n",
    "    routs = get_rollouts()\n",
    "    states,actions,rewards,advantages,returns,log_probs = process_rollouts(routs)\n",
    "\n",
    "    idxs = partition(shuffle(1:size(states)[end]),BATCH_SIZE)\n",
    "      \n",
    "    for epoch in 1:PPO_EPOCHS\n",
    "        for i in idxs\n",
    "            mb_states = states[:,i] \n",
    "            mb_actions = actions[:,i] \n",
    "            mb_advantages = advantages[:,i] \n",
    "            mb_returns = returns[:,i] \n",
    "            mb_log_probs = log_probs[:,i]\n",
    "            \n",
    "            trpo_update(mb_states,mb_actions,mb_advantages,mb_returns,mb_log_probs)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function train()\n",
    "    for i in 1:NUM_EPISODES\n",
    "        println(\"EP : $i\")\n",
    "        train_step()\n",
    "        println(\"Ep done\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using Distributions.params in module Main conflicts with an existing identifier.\n",
      "WARNING: using JLD.@load in module Main conflicts with an existing identifier.\n",
      "WARNING: using JLD.@save in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using Flux\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using Random\n",
    "using BSON:@save,@load\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "invalid redefinition of constant CategoricalPolicy",
     "output_type": "error",
     "traceback": [
      "invalid redefinition of constant CategoricalPolicy",
      "",
      "Stacktrace:",
      " [1] top-level scope at none:0"
     ]
    }
   ],
   "source": [
    "# TODO : DiagonalGaussian\n",
    "\n",
    "\"\"\"\n",
    "policies.jl\n",
    "\"\"\"\n",
    "\n",
    "mutable struct EnvWrap{T,V}\n",
    "    env::T\n",
    "    STATE_SIZE::V\n",
    "    ACTION_SIZE::V\n",
    "end\n",
    "\n",
    "function scale_rewards(env_wrap::EnvWrap,rewards)\n",
    "    if env_wrap.name == :Pendulum\n",
    "         rewards = rewards ./ 16.2736044 .+ 2.0f0\n",
    "    end\n",
    "    \n",
    "    rewards\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Define all policies here\n",
    "\"\"\"\n",
    "\n",
    "mutable struct CategoricalPolicy\n",
    "    π # Neural network for the policy\n",
    "    value_net # Neural network for the value function\n",
    "    env_wrap # A wrapper for environment variables\n",
    "end\n",
    "\n",
    "function CategoricalPolicy(env_wrap::EnvWrap, policy_net = nothing, value_net = nothing)\n",
    "    if policy_net == nothing\n",
    "        policy_net = Chain(Dense(\n",
    "                        env_wrap.STATE_SIZE,30,relu;initW = _random_normal,initb=constant_init),\n",
    "                        Dense(30,env_wrap.ACTION_SIZE;initW = _random_normal,initb=constant_init),\n",
    "                        x -> softmax(x))\n",
    "    end\n",
    "    \n",
    "    if value_net == nothing\n",
    "        value_net = Chain(Dense(env_wrap.STATE_SIZE,30,relu;initW=_random_normal),\n",
    "                  Dense(30,30,relu;initW=_random_normal),\n",
    "                  Dense(30,1;initW=_random_normal))\n",
    "    end\n",
    "    \n",
    "    return CategoricalPolicy(policy_net,value_net,env_wrap)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Define the following for each policy : \n",
    "    `action` : a function taking in the policy variable and giving a particular action according to the environemt\n",
    "    `log_prob` : a function giving the log probability of an action under the current policy parameters\n",
    "    `entropy` : a function defining the entropy of the policy distribution\n",
    "\n",
    "Populate each function with it's appropriate distribution\n",
    "\"\"\"\n",
    "\n",
    "function action(policy,state)\n",
    "    \"\"\"\n",
    "    policy : A policy type defined in `policy.jl`\n",
    "    state : output of reset!(env) or step!(env,action)\n",
    "    \"\"\"\n",
    "    \n",
    "    state = reshape(Array(state),length(state),1)\n",
    "    a = nothing\n",
    "    \n",
    "    if typeof(policy) <: CategoricalPolicy\n",
    "        action_probs = policy.π(state).data\n",
    "        action_probs = reshape(action_probs,policy.env_wrap.ACTION_SIZE)\n",
    "        a = sample(1:policy.env_wrap.ACTION_SIZE,Weights(action_probs)) - 1\n",
    "    else\n",
    "        error(\"Policy type not yet implemented\")\n",
    "    end\n",
    "    \n",
    "    a\n",
    "end\n",
    "\n",
    "function log_prob(policy,states,actions)\n",
    "    log_probs = []\n",
    "    \n",
    "    for i in 1:length(states)\n",
    "        if policy <: CategoricalPolicy\n",
    "            action_probs = policy.π(states[i])\n",
    "            prob = action_probs[actions[i],:].data\n",
    "            push!(log_probs,log.(prob))\n",
    "        else\n",
    "            error(\"Not Implemented\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    log_probs\n",
    "end\n",
    "\n",
    "function entropy(policy)\n",
    "    if policy <: CategoricalPolicy\n",
    "        return sum(policy.π .* log.(policy.π .+ 1f-10),dims=1)\n",
    "    else\n",
    "        error(\"Not Implemented\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_policy_params(policy)\n",
    "    if policy <: CategoricalPolicy\n",
    "         return params(policy.π)\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_policy_net(policy)\n",
    "    \"\"\"\n",
    "    Returns the policy neural network\n",
    "    \"\"\"\n",
    "    if policy <: CategoricalPolicy\n",
    "        return [policy.π]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: activating new environment at /media/shreyas/Data/GSoC/TRPO.jl/Project.toml.\n",
      "└ @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.1/Pkg/src/API.jl:519\n",
      "WARNING: using Distributions.params in module Main conflicts with an existing identifier.\n",
      "WARNING: using JLD.@load in module Main conflicts with an existing identifier.\n",
      "WARNING: using JLD.@save in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "collect_and_process_rollouts (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"../../Project.toml\")\n",
    "\n",
    "using Flux\n",
    "using Gym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using Random\n",
    "using BSON:@save,@load\n",
    "using JLD\n",
    "\n",
    "num_processes = 1\n",
    "include(\"../common/policies.jl\")\n",
    "include(\"../common/utils.jl\")\n",
    "include(\"../common/buffer.jl\")\n",
    "include(\"rollout.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0003, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initialize_episode_buffer()\n",
    "    eb = Buffer()\n",
    "    register(eb,\"states\")\n",
    "    register(eb,\"actions\")\n",
    "    register(eb,\"rewards\")\n",
    "    register(eb,\"next_states\")\n",
    "    register(eb,\"dones\")\n",
    "    register(eb,\"returns\")\n",
    "    register(eb,\"advantages\")\n",
    "    register(eb,\"log_probs\")\n",
    "    \n",
    "    return eb\n",
    "end\n",
    "\n",
    "function initialize_stats_buffer()\n",
    "    sb = Buffer()\n",
    "    register(sb,\"rollout_returns\")\n",
    "    \n",
    "    return sb\n",
    "end\n",
    "\n",
    "function get_policy(env_wrap::EnvWrap)\n",
    "    if typeof(env_wrap.env._env.action_space) <: Gym.Space.Discrete\n",
    "        return CategoricalPolicy(env_wrap)\n",
    "    elseif typeof(env_wrap.env._env.action_space) <: Gym.Space.Box\n",
    "        return DiagonalGaussianPolicy(env_wrap)\n",
    "    else\n",
    "        error(\"Policy type not supported\")\n",
    "    end\n",
    "end\n",
    "\n",
    "#----------------Hyperparameters----------------#\n",
    "# Environment Variables #\n",
    "ENV_NAME = \"CartPole-v0\"\n",
    "EPISODE_LENGTH = 100\n",
    "# Policy parameters #\n",
    "η = 3e-4 # Learning rate\n",
    "STD = 0.0 # Standard deviation\n",
    "# GAE parameters\n",
    "γ = 0.99\n",
    "λ = 0.95\n",
    "# Optimization parameters\n",
    "PPO_EPOCHS = 10\n",
    "NUM_EPISODES = 100000\n",
    "BATCH_SIZE = 256\n",
    "c₀ = 1.0\n",
    "c₁ = 1.0\n",
    "c₂ = 0.001\n",
    "# PPO parameters\n",
    "ϵ = 0.2\n",
    "# FREQUENCIES\n",
    "SAVE_FREQUENCY = 50\n",
    "VERBOSE_FREQUENCY = 5\n",
    "global_step = 0\n",
    "\n",
    "# Define policy\n",
    "env_wrap = EnvWrap(ENV_NAME)\n",
    "policy = get_policy(env_wrap)\n",
    "\n",
    "# Define buffers\n",
    "episode_buffer = initialize_episode_buffer()\n",
    "stats_buffer = initialize_stats_buffer()\n",
    "\n",
    "# Define optimizer\n",
    "opt = ADAM(η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(policy,states::Array,actions::Array,advantages::Array,returns::Array,old_log_probs::Array)\n",
    "    new_log_probs = log_prob(policy,states,actions)\n",
    "    \n",
    "    # Surrogate loss computations\n",
    "    ratio = exp.(new_log_probs .- old_log_probs)\n",
    "    surr1 = ratio .* advantages\n",
    "    surr2 = clamp.(ratio,(1.0 - ϵ),(1.0 + ϵ)) .* advantages\n",
    "    policy_loss = mean(min.(surr1,surr2))\n",
    "    \n",
    "    value_predicted = policy.value_net(states)\n",
    "    value_loss = mean((value_predicted .- returns).^2)\n",
    "    \n",
    "    entropy_loss = mean(entropy(policy,states))\n",
    "    \n",
    "    -c₀*policy_loss + c₁*value_loss - c₂*entropy_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ppo_update (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ppo_update(policy,states::Array,actions::Array,advantages::Array,returns::Array,old_log_probs::Array)\n",
    "    model_params = params(get_policy_params(policy)...,get_value_params(policy)...)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gs = Tracker.gradient(() -> loss(policy,states,actions,advantages,returns,old_log_probs),model_params)\n",
    "    \n",
    "    # Take a step of optimisation\n",
    "    update!(opt,model_params,gs)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_step (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_step()    \n",
    "    clear(episode_buffer)\n",
    "    collect_and_process_rollouts(policy,episode_buffer,EPISODE_LENGTH,stats_buffer)\n",
    "    \n",
    "    idxs = partition(shuffle(1:size(episode_buffer.exp_dict[\"states\"])[end]),BATCH_SIZE)\n",
    "    \n",
    "    for epoch in 1:PPO_EPOCHS\n",
    "        for i in idxs\n",
    "            mb_states = episode_buffer.exp_dict[\"states\"][:,i] \n",
    "            mb_actions = episode_buffer.exp_dict[\"actions\"][:,i] \n",
    "            mb_advantages = episode_buffer.exp_dict[\"advantages\"][:,i] \n",
    "            mb_returns = episode_buffer.exp_dict[\"returns\"][:,i] \n",
    "            mb_log_probs = episode_buffer.exp_dict[\"log_probs\"][:,i]\n",
    "            \n",
    "            ppo_update(policy,mb_states,mb_actions,mb_advantages,mb_returns,mb_log_probs)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train()\n",
    "    for i in 1:NUM_EPISODES\n",
    "        println(i)\n",
    "        train_step()\n",
    "        println(mean(stats_buffer.exp_dict[\"rollout_returns\"]))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10.733765292570691\n",
      "2\n",
      "8.73065678820008\n",
      "3\n",
      "8.666273437248606\n",
      "4\n",
      "9.609581457465662\n",
      "5\n",
      "10.091116830384955\n",
      "6\n",
      "10.053253043759009\n",
      "7\n",
      "9.578152363769075\n",
      "8\n",
      "9.335697576204243\n",
      "9\n",
      "9.197230719398695\n",
      "10\n",
      "10.492536008272543\n",
      "11\n",
      "10.514465943208739\n",
      "12\n",
      "10.460254957160451\n",
      "13\n",
      "10.243197965321219\n",
      "14\n",
      "10.057149115173305\n",
      "15\n",
      "9.835175726417049\n",
      "16\n",
      "9.612197191378899\n",
      "17\n",
      "9.627005245452452\n",
      "18\n",
      "9.41463988919647\n",
      "19\n",
      "9.24900195638648\n",
      "20\n",
      "9.572547373266705\n",
      "21\n",
      "9.586422932188734\n",
      "22\n",
      "9.371993354907653\n",
      "23\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] hcat(::Array{Float32,1}, ::Array{Float32,1}, ::Array{Float32,1}, ::Array{Float32,1}, ::Vararg{Array{Float32,1},N} where N) at ./array.jl:1527",
      " [2] collect_and_process_rollouts(::CategoricalPolicy, ::Buffer{Dict{Any,Any}}, ::Int64, ::Buffer{Dict{Any,Any}}) at /media/shreyas/Data/GSoC/TRPO.jl/src/ppo/rollout.jl:102",
      " [3] train_step() at ./In[5]:3",
      " [4] train() at ./In[6]:4",
      " [5] top-level scope at In[7]:1"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
