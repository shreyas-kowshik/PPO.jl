{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Rollouts\n",
    "- Define networks\n",
    "- GAE\n",
    "- Utils : Normal distributions and log probability of an action\n",
    "- train\n",
    "- logging utilities\n",
    "\n",
    "Experiment with shared network for both policy and value function\n",
    "\"\"\"\n",
    "\n",
    "using Flux, CuArrays\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using BSON:@save,@load\n",
    "using JLD\n",
    "\n",
    "\"\"\"\n",
    "A few intricacies : \n",
    "The policy is a Normal distribution and the `policy_net` outputs the `μ` and `logσ`.\n",
    "Each action is assumed to be independent of the others.\n",
    "Thus our covariance matrix is a diagonal matrix with each element representing the variance of\n",
    "taking a particular action.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Utilities\n",
    "\"\"\"\n",
    "# weight initialization\n",
    "function _random_normal(shape...)\n",
    "    return map(Float32,rand(Normal(0,0.1),shape...))\n",
    "end\n",
    "\n",
    "function constant_init(shape...)\n",
    "    return map(Float32,ones(shape...) * 0.1)\n",
    "end\n",
    "\n",
    "function normal_log_prob(μ,log_std,a)\n",
    "    \"\"\"\n",
    "    Returns the log probability of an action under a policy Gaussian policy π\n",
    "    \"\"\"\n",
    "    σ = exp.(log_std)\n",
    "    σ² = σ.^2\n",
    "    -(((a .- μ).^2)./(2.0 * σ²)) .- 0.5*log.(sqrt(2 * π)) .- log.(σ)\n",
    "end\n",
    "\n",
    "function normal_entropy(log_std)\n",
    "    0.5 + 0.5 * log(2 * π) .+ log_std\n",
    "end\n",
    "\n",
    "function normalise(arr)\n",
    "    (arr .- mean(arr))./(sqrt(var(arr) + 1e-10))\n",
    "end\n",
    "\n",
    "function scale_rewards(rewards)\n",
    "     rewards ./ 16.2736044\n",
    "end\n",
    "\n",
    "# Logging #\n",
    "\"\"\"\n",
    "Create logging utility\n",
    "\"\"\"\n",
    "mutable struct Logger\n",
    "    hist_dict\n",
    "end\n",
    "\n",
    "Logger() = Logger(Dict())\n",
    "\n",
    "function register(l::Logger,name::String)\n",
    "     l.hist_dict[name] = []\n",
    "end\n",
    "\n",
    "function add(l::Logger,name,value)\n",
    "    \"\"\"\n",
    "    Add a variable for it's history to be logged\n",
    "    \"\"\"\n",
    "    if !(name in l.hist_dict.keys)\n",
    "        err(\"Error...\")\n",
    "    else\n",
    "        push!(l.hist_dict[name],value)\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "# Policy parameters #\n",
    "η = 3e-4 # Learning rate\n",
    "STD = 0.0 # Standard deviation\n",
    "HIDDEN_SIZE = 256\n",
    "# Environment Variables #\n",
    "STATE_SIZE = 3\n",
    "ACTION_SIZE = 1\n",
    "MIN_RANGE = -2.0f0\n",
    "MAX_RANGE = 2.0f0\n",
    "EPISODE_LENGTH = 100\n",
    "TEST_STEPS = 10000\n",
    "# GAE parameters\n",
    "γ = 0.99\n",
    "λ = 0.95\n",
    "# Optimization parameters\n",
    "PPO_EPOCHS = 10\n",
    "NUM_EPISODES = 15000\n",
    "BATCH_SIZE = 5\n",
    "c₀ = 1.0\n",
    "c₁ = 0.5\n",
    "c₂ = 0.001\n",
    "# PPO parameters\n",
    "ϵ = 0.2\n",
    "# FREQUENCIES\n",
    "SAVE_FREQUENCY = 50\n",
    "VERBOSE_FREQUENCY = 5\n",
    "global_step = 0\n",
    "\n",
    "# Global variable to monitor losses\n",
    "reward_hist = []\n",
    "policy_l = 0.0\n",
    "entropy_l = 0.0\n",
    "value_l = 0.0\n",
    "\n",
    "# Checkpointing and resuming\n",
    "resume = false\n",
    "\n",
    "\"\"\"\n",
    "Define the networks\n",
    "\"\"\"\n",
    "\n",
    "policy_μ = nothing\n",
    "policy_Σ = nothing\n",
    "value_fn = nothing\n",
    "\n",
    "if resume == true\n",
    "    @load \"./weights/policy_mu.bson\" policy_μ\n",
    "    @load \"./weights/policy_sigma.bson\" policy_Σ\n",
    "    @load \"./weights/value.bson\" value\n",
    "    \n",
    "    value_fn = value\n",
    "    \n",
    "    println(\"Networks loaded from disk...\")\n",
    "else\n",
    "    policy_μ = Chain(Dense(STATE_SIZE,HIDDEN_SIZE,tanh;initW = _random_normal,initb=constant_init),\n",
    "                     Dense(HIDDEN_SIZE,ACTION_SIZE;initW = _random_normal,initb=constant_init),\n",
    "                     x->tanh.(x),\n",
    "                     x->param(2.0) .* x)\n",
    "    policy_Σ = param(ones(ACTION_SIZE) * STD)\n",
    "    \n",
    "    value_fn = Chain(Dense(STATE_SIZE,HIDDEN_SIZE,tanh),\n",
    "                 Dense(HIDDEN_SIZE,HIDDEN_SIZE,tanh),\n",
    "                  Dense(HIDDEN_SIZE,1))\n",
    "    \n",
    "    println(\"Initialized Networks\")\n",
    "end\n",
    "\n",
    "# Optimizer\n",
    "opt = ADAM(η)\n",
    "\n",
    "function action(state)\n",
    "    # Acccounting for the element type\n",
    "    state = reshape(Array(state),length(state),1) \n",
    "\n",
    "    # Our policy outputs the parameters of a Normal distribution\n",
    "    μ = policy_μ(state)\n",
    "    μ = reshape(μ,ACTION_SIZE) |> cpu\n",
    "    log_std = policy_Σ |> cpu\n",
    "    \n",
    "    σ² = (exp.(log_std)).^2\n",
    "    Σ = diagm(0=>σ².data)\n",
    "    \n",
    "    dis = MvNormal(μ.data,Σ)\n",
    "    \n",
    "    a = rand(dis,ACTION_SIZE)\n",
    "    a\n",
    "end\n",
    "\n",
    "function run_episode(env)\n",
    "    experience = []\n",
    "    \n",
    "    s = reset!(env)\n",
    "    for i in 1:EPISODE_LENGTH\n",
    "        a = action(s)\n",
    "        a = convert.(Float64,a)\n",
    "        a = reshape(a,ACTION_SIZE)\n",
    "        \n",
    "        r,s_ = step!(env,a)\n",
    "        push!(experience,(s,a,r,s_))\n",
    "        s = s_\n",
    "        if env.done\n",
    "           break \n",
    "        end\n",
    "    end\n",
    "    experience\n",
    "end\n",
    "\n",
    "function test_run(env)\n",
    "    ep_r = 0.0\n",
    "    \n",
    "    s = reset!(env)\n",
    "    for i in 1:TEST_STEPS\n",
    "        OpenAIGym.render(env)\n",
    "        a = policy_μ(s).data\n",
    "        a = convert.(Float64,a)\n",
    "        a = reshape(a,ACTION_SIZE)\n",
    "        println(\"Action : $a\")\n",
    "        \n",
    "        r,s_ = step!(env,a)\n",
    "        ep_r += r\n",
    "        \n",
    "        s = s_\n",
    "        if env.done\n",
    "           break \n",
    "        end\n",
    "    end\n",
    "    ep_r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rollout collection\n",
    "\"\"\"\n",
    "num_processes = 9\n",
    "addprocs(num_processes) \n",
    "\n",
    "@everywhere function collect(env)\n",
    "    run_episode(env)\n",
    "end\n",
    "\n",
    "@everywhere function rollout()\n",
    "  env = GymEnv(:Pendulum,:v0)\n",
    "  return collect(env)\n",
    "end\n",
    "\n",
    "function get_rollouts()\n",
    "    g = []\n",
    "    for  w in workers()\n",
    "      push!(g, rollout())\n",
    "    end\n",
    "\n",
    "    fetch.(g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function gae(states,actions,rewards,next_states)\n",
    "    \"\"\"\n",
    "    Returns a Generalized Advantage Estimate for an episode\n",
    "    \"\"\"\n",
    "    Â = []\n",
    "    A = 0.0\n",
    "    for i in reverse(1:length(states))\n",
    "        δ = rewards[i] + γ*cpu.(value_fn(next_states[i]).data[1]) - cpu.(value_fn(states[i]).data[1])\n",
    "        A = δ + (γ*λ*A)\n",
    "        push!(Â,A)\n",
    "    end\n",
    "    \n",
    "    Â = reverse(Â)\n",
    "    return Â\n",
    "end\n",
    "\n",
    "function disconunted_returns(rewards)\n",
    "    r = 0.0\n",
    "    returns = []\n",
    "    for i in reverse(1:length(rewards))\n",
    "        r = rewards[i] + γ*r\n",
    "        push!(returns,r)\n",
    "    end\n",
    "    returns = reverse(returns)\n",
    "    returns\n",
    "end\n",
    "\n",
    "function log_prob_from_actions(states,actions)\n",
    "    \"\"\"\n",
    "    Returns log probabilities of the actions taken\n",
    "    \n",
    "    states,actions : episode vairbles in the form of a list\n",
    "    \"\"\"\n",
    "    log_probs = []\n",
    "    \n",
    "    for i in 1:length(states)\n",
    "        μ = reshape(policy_μ(states[i]),ACTION_SIZE).data\n",
    "        logΣ = policy_Σ.data |> cpu\n",
    "        push!(log_probs,normal_log_prob(μ,logΣ,actions[i]))\n",
    "    end\n",
    "    \n",
    "    log_probs\n",
    "end\n",
    "\n",
    "function process_rollouts(rollouts)\n",
    "    \"\"\"\n",
    "    rollouts : variable returned by calling `get_rollouts`\n",
    "    \n",
    "    Returns : \n",
    "    states, actions, rewards for minibatch processing\n",
    "    \"\"\"\n",
    "    # Process the variables\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    advantages = []\n",
    "    returns = []\n",
    "    log_probs = []\n",
    "    \n",
    "    # Logging statistics\n",
    "    episode_mean_returns = []\n",
    "    \n",
    "    for ro in rollouts\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        episode_next_states = []\n",
    "        \n",
    "        for i in 1:length(ro)\n",
    "             push!(episode_states,Array(ro[i][1]))\n",
    "             push!(episode_actions,ro[i][2])\n",
    "             push!(episode_rewards,ro[i][3])\n",
    "             push!(episode_next_states,ro[i][4])\n",
    "        end\n",
    "        \n",
    "        episode_rewards = scale_rewards(episode_rewards)\n",
    "        episode_advantages = gae(episode_states,episode_actions,episode_rewards,episode_next_states)\n",
    "        # episode_rewards = normalise(episode_rewards)\n",
    "        \n",
    "        episode_returns = disconunted_returns(episode_rewards)\n",
    "        \n",
    "        push!(episode_mean_returns,mean(episode_returns))\n",
    "        \n",
    "        push!(states,episode_states)\n",
    "        push!(actions,episode_actions)\n",
    "        push!(rewards,episode_rewards)\n",
    "        push!(advantages,episode_advantages)\n",
    "        push!(returns,episode_returns)\n",
    "        push!(log_probs,log_prob_from_actions(episode_states,episode_actions))\n",
    "    end\n",
    "    \n",
    "    states = cat(states...,dims=1)\n",
    "    actions = cat(actions...,dims=1)\n",
    "    rewards = cat(rewards...,dims=1)\n",
    "    advantages = cat(advantages...,dims=1)\n",
    "    returns = cat(returns...,dims=1)\n",
    "    log_probs = cat(log_probs...,dims=1)\n",
    "    \n",
    "    push!(reward_hist,mean(episode_mean_returns))\n",
    "    \n",
    "    if length(reward_hist) <= 100\n",
    "        println(\"RETURNS : $(mean(episode_mean_returns))\")\n",
    "    else\n",
    "        println(\"MEAN RETURNS : $(mean(reward_hist))\")\n",
    "        println(\"LAST 100 RETURNS : $(mean(reward_hist[end-100:end]))\")\n",
    "    end\n",
    "    \n",
    "    return hcat(states...),hcat(actions...),hcat(rewards...),hcat(advantages...),hcat(returns...),hcat(log_probs...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function print_losses(pl,vl,el) \n",
    "   println(\"------\")\n",
    "   println(\"Policy Loss : $pl\")\n",
    "   println(\"Value Loss : $vl\")\n",
    "   println(\"Entropy Loss : $el\") \n",
    "   println(\"------\")\n",
    "end\n",
    "\n",
    "function loss(states,actions,advantages,returns,old_log_probs)\n",
    "    global global_step,policy_l,entropy_l,value_l\n",
    "    global_step += 1\n",
    "    \n",
    "    μ = policy_μ(states)\n",
    "    logΣ = policy_Σ \n",
    "\n",
    "    new_log_probs = normal_log_prob(μ,logΣ,actions)\n",
    "\n",
    "    # Surrogate loss computation\n",
    "    ratio = exp.(new_log_probs .- old_log_probs)\n",
    "    surr1 = ratio .* advantages\n",
    "    surr2 = clamp.(ratio,(1.0 - ϵ),(1.0 + ϵ)) .* advantages\n",
    "    policy_loss = mean(min.(surr1,surr2))\n",
    "    \n",
    "    value_predicted = value_fn(states)\n",
    "    value_loss = mean((value_predicted .- returns).^2)\n",
    "\n",
    "    entropy_loss = mean(normal_entropy(logΣ))\n",
    "    \n",
    "    policy_l = policy_loss.data\n",
    "    entropy_l = entropy_loss.data\n",
    "    value_l = value_loss.data\n",
    "    \n",
    "    -c₀*policy_loss + c₁*value_loss - c₂*entropy_loss\n",
    "end\n",
    "\n",
    "function ppo_update(states,actions,advantages,returns,old_log_probs)\n",
    "    # Define model parameters\n",
    "    model_params = params(params(policy_μ)...,params(policy_Σ)...,params(value_fn)...)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gs = Tracker.gradient(() -> loss(states,actions,advantages,returns,old_log_probs),model_params)\n",
    "\n",
    "    g = gs[policy_μ.layers[1].W]\n",
    "    # println(\"GRAD : $(mean(g))\")\n",
    "    \n",
    "    # Take a step of optimisation\n",
    "    update!(opt,model_params,gs)\n",
    "end\n",
    "\n",
    "function train_step()    \n",
    "    routs = get_rollouts()\n",
    "    states,actions,rewards,advantages,returns,log_probs = process_rollouts(routs)\n",
    "    \n",
    "    idxs = partition(1:size(states)[end],BATCH_SIZE)\n",
    "    \n",
    "    for epoch in 1:PPO_EPOCHS\n",
    "        for i in idxs\n",
    "            mb_states = states[:,i] \n",
    "            mb_actions = actions[:,i] \n",
    "            mb_advantages = advantages[:,i] \n",
    "            mb_returns = returns[:,i] \n",
    "            mb_log_probs = log_probs[:,i]\n",
    "            \n",
    "            ppo_update(mb_states,mb_actions,mb_advantages,mb_returns,mb_log_probs)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function train()\n",
    "    for i in 1:NUM_EPISODES\n",
    "        println(\"EP : $i\")\n",
    "        train_step()\n",
    "        println(\"Ep done\")\n",
    "        \n",
    "        if i%300 == 0\n",
    "            if η > 1e-6\n",
    "                η = η / 3.0\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if i % VERBOSE_FREQUENCY == 0\n",
    "            # Show important statistics\n",
    "            println(\"-----___Stats___-----\")\n",
    "            println(\"Entropy : $(normal_entropy(policy_Σ))\")\n",
    "            println(\"Policy Loss : $(policy_l)\")\n",
    "            # println(\"Entropy Loss : $(entropy_l)\")\n",
    "            println(\"Value Loss : $(value_l)\")\n",
    "        end\n",
    "        \n",
    "        if i%SAVE_FREQUENCY == 0\n",
    "            @save \"weights/policy_mu.bson\" policy_μ\n",
    "            @save \"weights/policy_sigma.bson\" policy_Σ\n",
    "            @save \"weights/value.bson\" value_fn\n",
    "            \n",
    "            save(\"stats.jld\",\"rewards\",reward_hist)\n",
    "            println(\"\\n\\n\\n----MAX REWRD SO FAR : $(maximum(reward_hist))---\\n\\n\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the policy #\n",
    "env = GymEnv(\"Pendulum-v0\")\n",
    "env.pyenv._max_episode_steps = 50000\n",
    "TEST_STEPS = 50000\n",
    "\n",
    "r = test_run(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_base(Array(ro[1][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function loss(a)\n",
    "    r = exp.(a .- (ones(size(a)) |> gpu))\n",
    "    s1 = r .* (rand(size(a)) |> gpu)\n",
    "    s2 = clamp.(a,0.9,1.1)\n",
    "    -1.0 * mean(min.(s1,s2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = Tracker.gradient(() -> loss(out),params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs[m.layers[1].W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Flux,CuArrays\n",
    "using Flux:Tracker\n",
    "using Statistics\n",
    "\n",
    "m = Chain(Conv((3,3),3=>64)) |> gpu\n",
    "x = rand(256,256,3,1) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function loss(x)\n",
    "   out = m(x)\n",
    "   mean(out)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function test()\n",
    "    @time o = loss(x)\n",
    "    @time gs = Tracker.gradient(() -> loss(x),params(m))\n",
    "end\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Flux,CuArrays\n",
    "using Flux:Tracker\n",
    "using Flux:@treelike\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct Net\n",
    "    u\n",
    "end\n",
    "\n",
    "@treelike Net\n",
    "\n",
    "function Net()\n",
    "   Net(Chain(Conv((3,3),3=>64))) \n",
    "end\n",
    "\n",
    "function (n::Net)(x)\n",
    "   return n.u(x) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Net() |> gpu\n",
    "x = rand(256,256,3,1) |> gpu\n",
    "\n",
    "function loss(x)\n",
    "   out = m(x)\n",
    "   mean(out)\n",
    "end\n",
    "\n",
    "function test()\n",
    "    @time o = loss(x)\n",
    "    @time gs = Tracker.gradient(() -> loss(x),params(m))\n",
    "end\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = param(ones(2)) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reward_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean(reward_hist[end-100-100:end-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD\n",
    "save(\"stats.jld\",\"rewards\",reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rh = load(\"stats.jld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"~/envs/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "if i == 1\n",
    "    a = 3\n",
    "else\n",
    "    b = 5\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using OpenAIGym\n",
    "env = GymEnv(:Pendulum, :v0)\n",
    "for i ∈ 1:20\n",
    "  T = 0\n",
    "  R = run_episode(env, RandomPolicy()) do (s, a, r, s′)\n",
    "    render(env)\n",
    "    T += 1\n",
    "  end\n",
    "  @info(\"Episode $i finished after $T steps. Total reward: $R\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test\n",
    "\"\"\"\n",
    "\n",
    "using Flux, CuArrays\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using BSON:@save,@load\n",
    "using JLD\n",
    "\n",
    "ACTION_SIZE = 1\n",
    "TEST_STEPS = 50000\n",
    "\n",
    "# Load the policy\n",
    "@load \"./weights/policy_mu.bson\" policy_μ\n",
    "@load \"./weights/policy_sigma.bson\" policy_Σ\n",
    "\n",
    "# Test Run Function\n",
    "function test_run(env)\n",
    "    ep_r = 0.0\n",
    "    \n",
    "    s = reset!(env)\n",
    "    for i in 1:TEST_STEPS\n",
    "        if i % 10000 == 0\n",
    "            println(\"Resetting...\")\n",
    "            s = reset!(env)\n",
    "        end\n",
    "        OpenAIGym.render(env)\n",
    "        a = policy_μ(s).data\n",
    "        a = convert.(Float64,a)\n",
    "        a = reshape(a,ACTION_SIZE)\n",
    "\n",
    "        r,s_ = step!(env,a)\n",
    "        ep_r += r\n",
    "        \n",
    "        s = s_\n",
    "        if env.done\n",
    "           break \n",
    "        end\n",
    "    end\n",
    "    ep_r\n",
    "end\n",
    "\n",
    "env = GymEnv(\"Pendulum-v0\")\n",
    "env.pyenv._max_episode_steps = TEST_STEPS\n",
    "\n",
    "r = test_run(env)\n",
    "println(\"---Total Steps : $TEST_STEPS ::: Total Reward : $r---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OpenAIGym.render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OpenAIGym.render(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp.(policy_Σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ro = get_rollouts()\n",
    "ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value(Array(reset!(env)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ro = get_rollouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s,a,r,ad,re,lp = process_rollouts(ro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.637294 + 0.152403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minimum(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Flux, CuArrays\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using BSON:@save,@load\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function check()\n",
    "    p = Chain(Dense(2,3))\n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = check()\n",
    "model1 = check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function change(m)\n",
    "    m.layers[1].W = m.layers[1].W .+ 1.0\n",
    "    return m\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = change(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[1].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Flux is only supported with CuArrays v0.9.\n",
      "│ Try running `] pin CuArrays@0.9`.\n",
      "└ @ Flux.CUDA /home/shreyas/.julia/packages/Flux/WSB7k/src/cuda/cuda.jl:12\n",
      "WARNING: using Distributions.params in module Main conflicts with an existing identifier.\n",
      "WARNING: using JLD.@load in module Main conflicts with an existing identifier.\n",
      "WARNING: using JLD.@save in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux, CuArrays\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using BSON:@save,@load\n",
    "using JLD\n",
    "\n",
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "# Policy parameters #\n",
    "η = 3e-4 # Learning rate\n",
    "STD = 0.0 # Standard deviation\n",
    "HIDDEN_SIZE = 256\n",
    "# Environment Variables #\n",
    "STATE_SIZE = 3\n",
    "ACTION_SIZE = 1\n",
    "MIN_RANGE = -2.0f0\n",
    "MAX_RANGE = 2.0f0\n",
    "EPISODE_LENGTH = 100\n",
    "TEST_STEPS = 10000\n",
    "# GAE parameters\n",
    "γ = 0.99\n",
    "λ = 0.95\n",
    "# Optimization parameters\n",
    "PPO_EPOCHS = 10\n",
    "NUM_EPISODES = 15000\n",
    "BATCH_SIZE = 5\n",
    "c₀ = 1.0\n",
    "c₁ = 0.5\n",
    "c₂ = 0.001\n",
    "# PPO parameters\n",
    "ϵ = 0.2\n",
    "# FREQUENCIES\n",
    "SAVE_FREQUENCY = 50\n",
    "VERBOSE_FREQUENCY = 5\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"./policy.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_μ,policy_Σ = gaussian_policy(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = categorical_policy(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value = value_fn(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE,tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ro = get_rollouts()\n",
    "s,a,r,ad,re,_ = process_rollouts(ro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ones(1,900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[:,1:450] .= 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = rand(2,900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "η = 0.01\n",
    "opt = ADAM(η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "η = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt.eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = #GymEnv#1(::Type, ::Type, ::Symbol, ::Symbol) at OpenAIGym.jl:49\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:49\n",
      "┌ Warning: `getindex(o::PyObject, s::AbstractString)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.\"s\"` instead of `o[\"s\"]`.\n",
      "│   caller = GymEnv(::Symbol, ::Symbol, ::PyCall.PyObject, ::Type) at OpenAIGym.jl:56\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:56\n",
      "┌ Warning: `getindex(o::PyObject, s::AbstractString)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.\"s\"` instead of `o[\"s\"]`.\n",
      "│   caller = GymEnv{PyCall.PyArray{Float64,1}}(::Symbol, ::Symbol, ::PyCall.PyObject, ::PyCall.PyObject, ::PyCall.PyArray{Float64,1}) at OpenAIGym.jl:34\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:34\n",
      "┌ Warning: `getindex(o::PyObject, s::AbstractString)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.\"s\"` instead of `o[\"s\"]`.\n",
      "│   caller = GymEnv{PyCall.PyArray{Float64,1}}(::Symbol, ::Symbol, ::PyCall.PyObject, ::PyCall.PyObject, ::PyCall.PyArray{Float64,1}) at OpenAIGym.jl:34\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:34\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = actions(::GymEnv{PyCall.PyArray{Float64,1}}, ::Nothing) at OpenAIGym.jl:117\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:117\n",
      "┌ Warning: `haskey(o::PyObject, s::Union{Symbol, AbstractString})` is deprecated, use `hasproperty(o, s)` instead.\n",
      "│   caller = actionset(::PyCall.PyObject) at OpenAIGym.jl:88\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:88\n",
      "┌ Warning: `haskey(o::PyObject, s::Union{Symbol, AbstractString})` is deprecated, use `hasproperty(o, s)` instead.\n",
      "│   caller = actionset(::PyCall.PyObject) at OpenAIGym.jl:91\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:91\n",
      "┌ Warning: `haskey(o::PyObject, s::Union{Symbol, AbstractString})` is deprecated, use `hasproperty(o, s)` instead.\n",
      "│   caller = actionset(::PyCall.PyObject) at OpenAIGym.jl:95\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:95\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = actionset(::PyCall.PyObject) at OpenAIGym.jl:97\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:97\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = actionset(::PyCall.PyObject) at OpenAIGym.jl:97\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GymEnv Pendulum-v0\n",
       "  TimeLimit\n",
       "  r  = 0.0\n",
       "  ∑r = 0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `haskey(o::PyObject, s::Union{Symbol, AbstractString})` is deprecated, use `hasproperty(o, s)` instead.\n",
      "│   caller = show(::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}}, ::GymEnv{PyCall.PyArray{Float64,1}}) at OpenAIGym.jl:64\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:64\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = show(::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}}, ::GymEnv{PyCall.PyArray{Float64,1}}) at OpenAIGym.jl:65\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:65\n"
     ]
    }
   ],
   "source": [
    "env = GymEnv(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
