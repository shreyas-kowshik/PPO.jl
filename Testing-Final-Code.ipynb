{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Flux is only supported with CuArrays v0.9.\n",
      "│ Try running `] pin CuArrays@0.9`.\n",
      "└ @ Flux.CUDA /home/shreyas/.julia/packages/Flux/WSB7k/src/cuda/cuda.jl:12\n",
      "WARNING: using Distributions.params in module Main conflicts with an existing identifier.\n",
      "WARNING: using JLD.@load in module Main conflicts with an existing identifier.\n",
      "WARNING: using JLD.@save in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "value_fn (generic function with 2 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------__CHECKING__------------------- #\n",
    "using Flux, CuArrays\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using BSON:@save,@load\n",
    "using JLD\n",
    "\n",
    "include(\"policy.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "# Environment Creation #\n",
    "env_name = \"CartPole-v0\"\n",
    "MODE = \"CAT\" # Can be either \"CON\" (Continuous) or \"CON\" (Categorical)\n",
    "\n",
    "# Environment Variables #\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = 2\n",
    "MIN_RANGE = -2.0f0\n",
    "MAX_RANGE = 2.0f0\n",
    "EPISODE_LENGTH = 100\n",
    "TEST_STEPS = 10000\n",
    "# Policy parameters #\n",
    "η = 3e-4 # Learning rate\n",
    "STD = 0.0 # Standard deviation\n",
    "HIDDEN_SIZE = 256\n",
    "# GAE parameters\n",
    "γ = 0.99\n",
    "λ = 0.95\n",
    "# Optimization parameters\n",
    "PPO_EPOCHS = 10\n",
    "NUM_EPISODES = 15000\n",
    "BATCH_SIZE = 5\n",
    "c₀ = 1.0\n",
    "c₁ = 0.5\n",
    "c₂ = 0.001\n",
    "# PPO parameters\n",
    "ϵ = 0.2\n",
    "# FREQUENCIES\n",
    "SAVE_FREQUENCY = 50\n",
    "VERBOSE_FREQUENCY = 5\n",
    "global_step = 0\n",
    "\n",
    "# Global variable to monitor losses\n",
    "reward_hist = []\n",
    "policy_l = 0.0\n",
    "entropy_l = 0.0\n",
    "value_l = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scale_rewards (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scale_rewards(rewards)\n",
    "    return rewards # ./ 16.2736044\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0003, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define the networks\n",
    "\"\"\"\n",
    "\n",
    "if MODE == \"CON\"\n",
    "\tpolicy_μ,policy_Σ = gaussian_policy(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE)\n",
    "\tvalue = value_fn(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE,tanh)\n",
    "elseif MODE == \"CAT\"\n",
    "\tpolicy = categorical_policy(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE)\n",
    "\tvalue = value_fn(STATE_SIZE,HIDDEN_SIZE,ACTION_SIZE,relu)\n",
    "else \n",
    "\terror(\"MODE can only be (CON) or (CAT)...\")\n",
    "end\n",
    "\n",
    "opt = ADAM(η)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_episode (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Functions to get rollouts\n",
    "\"\"\"\n",
    "\n",
    "function action(state)\n",
    "    # Acccounting for the element type\n",
    "    state = reshape(Array(state),length(state),1) \n",
    "\n",
    "    a = nothing\n",
    "    if MODE == \"CON\"\n",
    "\t    # Our policy outputs the parameters of a Normal distribution\n",
    "\t    μ = policy_μ(state)\n",
    "\t    μ = reshape(μ,ACTION_SIZE)\n",
    "\t    log_std = policy_Σ\n",
    "\t    \n",
    "\t    σ² = (exp.(log_std)).^2\n",
    "\t    Σ = diagm(0=>σ².data)\n",
    "\t    \n",
    "\t    dis = MvNormal(μ.data,Σ)\n",
    "\t    \n",
    "\t    a = rand(dis,ACTION_SIZE)\n",
    "\telse\n",
    "\t\taction_probs = policy(state).data\n",
    "        action_probs = reshape(action_probs,ACTION_SIZE)\n",
    "    \ta = sample(1:ACTION_SIZE,Weights(action_probs)) - 1\n",
    "    end\n",
    "    a\n",
    "end\n",
    "\n",
    "function run_episode(env)\n",
    "    experience = []\n",
    "    \n",
    "    s = reset!(env)\n",
    "    for i in 1:EPISODE_LENGTH\n",
    "        a = action(s)\n",
    "        # a = convert.(Float64,a)\n",
    "        \n",
    "        if MODE == \"CON\"\n",
    "            a = reshape(a,ACTION_SIZE)\n",
    "        end\n",
    "\n",
    "        r,s_ = step!(env,a)\n",
    "        push!(experience,(s,a,r,s_))\n",
    "        s = s_\n",
    "        if env.done\n",
    "           break \n",
    "        end\n",
    "    end\n",
    "    experience\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_rollouts (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-threaded parallel rollout collection\n",
    "\"\"\"\n",
    "\n",
    "num_processes = 9\n",
    "addprocs(num_processes) \n",
    "\n",
    "@everywhere function collect(env)\n",
    "    run_episode(env)\n",
    "end\n",
    "\n",
    "@everywhere function rollout()\n",
    "  env = GymEnv(env_name)\n",
    "  return collect(env)\n",
    "end\n",
    "\n",
    "function get_rollouts()\n",
    "    g = []\n",
    "    for  w in workers()\n",
    "      push!(g, rollout())\n",
    "    end\n",
    "\n",
    "    fetch.(g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_prob_from_actions"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generalized Adavantage Estimation\n",
    "\"\"\"\n",
    "\n",
    "function gae(states,actions,rewards,next_states)\n",
    "    \"\"\"\n",
    "    Returns a Generalized Advantage Estimate for an episode\n",
    "    \"\"\"\n",
    "    Â = []\n",
    "    A = 0.0\n",
    "    for i in reverse(1:length(states))\n",
    "        δ = rewards[i] + γ*cpu.(value(next_states[i]).data[1]) - cpu.(value(states[i]).data[1])\n",
    "        A = δ + (γ*λ*A)\n",
    "        push!(Â,A)\n",
    "    end\n",
    "    \n",
    "    Â = reverse(Â)\n",
    "    return Â\n",
    "end\n",
    "\n",
    "function disconunted_returns(rewards)\n",
    "    r = 0.0\n",
    "    returns = []\n",
    "    for i in reverse(1:length(rewards))\n",
    "        r = rewards[i] + γ*r\n",
    "        push!(returns,r)\n",
    "    end\n",
    "    returns = reverse(returns)\n",
    "    returns\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Calculate Log Probabilities\n",
    "\"\"\"\n",
    "function log_prob_from_actions(states,actions)\n",
    "    \"\"\"\n",
    "    Returns log probabilities of the actions taken\n",
    "    \n",
    "    states,actions : episode vairbles in the form of a list\n",
    "    \"\"\"\n",
    "    log_probs = []\n",
    "    \n",
    "    for i in 1:length(states)\n",
    "    \tif MODE == \"CON\"\n",
    "\t        μ = reshape(policy_μ(states[i]),ACTION_SIZE).data\n",
    "\t        logΣ = policy_Σ.data |> cpu\n",
    "        \tpush!(log_probs,normal_log_prob(μ,logΣ,actions[i]))\n",
    "        else\n",
    "        \taction_probs = policy(states[i])\n",
    "        \tprob = action_probs[actions[i],:].data\n",
    "        \tpush!(log_probs,log.(prob))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    log_probs\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ppo_update"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process and extraction information from rollouts\n",
    "\"\"\"\n",
    "\n",
    "function process_rollouts(rollouts)\n",
    "    \"\"\"\n",
    "    rollouts : variable returned by calling `get_rollouts`\n",
    "    \n",
    "    Returns : \n",
    "    states, actions, rewards for minibatch processing\n",
    "    \"\"\"\n",
    "    # Process the variables\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    advantages = []\n",
    "    returns = []\n",
    "    log_probs = []\n",
    "    \n",
    "    # Logging statistics\n",
    "    episode_mean_returns = []\n",
    "    \n",
    "    for ro in rollouts\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        episode_next_states = []\n",
    "        \n",
    "        for i in 1:length(ro)\n",
    "             push!(episode_states,Array(ro[i][1]))\n",
    "             \n",
    "             if MODE == \"CON\"\n",
    "                 push!(episode_actions,ro[i][2])\n",
    "             else\n",
    "                 push!(episode_actions,ro[i][2] + 1)\n",
    "             end\n",
    "             \n",
    "             push!(episode_rewards,ro[i][3])\n",
    "             push!(episode_next_states,ro[i][4])\n",
    "        end\n",
    "        \n",
    "        episode_rewards = scale_rewards(episode_rewards)\n",
    "        episode_advantages = gae(episode_states,episode_actions,episode_rewards,episode_next_states)\n",
    "        # episode_rewards = normalise(episode_rewards)\n",
    "        \n",
    "        episode_returns = disconunted_returns(episode_rewards)\n",
    "        \n",
    "        push!(episode_mean_returns,mean(episode_returns))\n",
    "        \n",
    "        push!(states,episode_states)\n",
    "        push!(actions,episode_actions)\n",
    "        push!(rewards,episode_rewards)\n",
    "        push!(advantages,episode_advantages)\n",
    "        push!(returns,episode_returns)\n",
    "        push!(log_probs,log_prob_from_actions(episode_states,episode_actions))\n",
    "    end\n",
    "    \n",
    "    states = cat(states...,dims=1)\n",
    "    actions = cat(actions...,dims=1)\n",
    "    rewards = cat(rewards...,dims=1)\n",
    "    advantages = cat(advantages...,dims=1)\n",
    "    returns = cat(returns...,dims=1)\n",
    "    log_probs = cat(log_probs...,dims=1)\n",
    "    \n",
    "    push!(reward_hist,mean(episode_mean_returns))\n",
    "    \n",
    "    if length(reward_hist) <= 100\n",
    "        println(\"RETURNS : $(mean(episode_mean_returns))\")\n",
    "    else\n",
    "        println(\"MEAN RETURNS : $(mean(reward_hist))\")\n",
    "        println(\"LAST 100 RETURNS : $(mean(reward_hist[end-100:end]))\")\n",
    "    end\n",
    "    \n",
    "    return hcat(states...),hcat(actions...),hcat(rewards...),hcat(advantages...),hcat(returns...),hcat(log_probs...)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Loss function definition\n",
    "\"\"\"\n",
    "function loss(states,actions,advantages,returns,old_log_probs)\n",
    "    global global_step,policy_l,entropy_l,value_l\n",
    "    global_step += 1\n",
    "    \n",
    "    if MODE == \"CON\"\n",
    "\t    μ = policy_μ(states)\n",
    "\t    logΣ = policy_Σ \n",
    "        \n",
    "\t    new_log_probs = normal_log_prob(μ,logΣ,actions)\n",
    "\telse\n",
    "\t\taction_probs = policy(states) # ACTION_SIZE x BATCH_SIZE\n",
    "\t\tactions_one_hot = zeros(ACTION_SIZE,size(action_probs)[end])\n",
    "        \n",
    "\t\tfor i in 1:size(action_probs)[end]\n",
    "\t\t\tactions_one_hot[actions[:,i][1],i] = 1.0\t\t\t\t\n",
    "\t\tend\n",
    "        \n",
    "\t\tnew_log_probs = log.(sum((action_probs .+ 1f-5) .* actions_one_hot,dims=1))\n",
    "    end\n",
    "    \n",
    "    # Surrogate loss computations\n",
    "    ratio = exp.(new_log_probs .- old_log_probs)\n",
    "    surr1 = ratio .* advantages\n",
    "    surr2 = clamp.(ratio,(1.0 - ϵ),(1.0 + ϵ)) .* advantages\n",
    "    policy_loss = mean(min.(surr1,surr2))\n",
    "    \n",
    "    value_predicted = value(states)\n",
    "    value_loss = mean((value_predicted .- returns).^2)\n",
    "    \n",
    "    if MODE == \"CON\"\n",
    "        entropy_loss = mean(normal_entropy(logΣ))\n",
    "    else\n",
    "        entropy_loss = mean(categorical_entropy(action_probs))\n",
    "    end\n",
    "    \n",
    "    policy_l = policy_loss.data\n",
    "    entropy_l = entropy_loss.data\n",
    "    value_l = value_loss.data\n",
    "    \n",
    "    -c₀*policy_loss + c₁*value_loss - c₂*entropy_loss\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Optimization Function\n",
    "\"\"\"\n",
    "function ppo_update(states,actions,advantages,returns,old_log_probs)\n",
    "    # Define model parameters\n",
    "    if MODE == \"CON\"\n",
    "        model_params = params(params(policy_μ)...,params(policy_Σ)...,params(value)...)\n",
    "    else\n",
    "        model_params = params(params(policy)...,params(value)...)\n",
    "    end\n",
    "\n",
    "    # Calculate gradients\n",
    "    gs = Tracker.gradient(() -> loss(states,actions,advantages,returns,old_log_probs),model_params)\n",
    "\n",
    "    # Take a step of optimisation\n",
    "    update!(opt,model_params,gs)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train\n",
    "\"\"\"\n",
    "\n",
    "function train_step()    \n",
    "    routs = get_rollouts()\n",
    "    states,actions,rewards,advantages,returns,log_probs = process_rollouts(routs)\n",
    "    \n",
    "    idxs = partition(1:size(states)[end],BATCH_SIZE)\n",
    "    \n",
    "    for epoch in 1:PPO_EPOCHS\n",
    "        for i in idxs\n",
    "            mb_states = states[:,i] \n",
    "            mb_actions = actions[:,i] \n",
    "            mb_advantages = advantages[:,i] \n",
    "            mb_returns = returns[:,i] \n",
    "            mb_log_probs = log_probs[:,i]\n",
    "            \n",
    "            ppo_update(mb_states,mb_actions,mb_advantages,mb_returns,mb_log_probs)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function train()\n",
    "    for i in 1:NUM_EPISODES\n",
    "        println(\"EP : $i\")\n",
    "        train_step()\n",
    "        println(\"Ep done\")\n",
    "        \n",
    "        # Anneal learning rate\n",
    "        if i%300 == 0\n",
    "            if opt.eta > 1e-6\n",
    "                opt.eta = opt.eta / 3.0\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        if i % VERBOSE_FREQUENCY == 0\n",
    "            # Show important statistics\n",
    "            println(\"-----___Stats___-----\")\n",
    "            \n",
    "            if MODE == \"CON\"\n",
    "                println(\"Entropy : $(normal_entropy(policy_Σ))\")\n",
    "            end\n",
    "            \n",
    "            println(\"Policy Loss : $(policy_l)\")\n",
    "            println(\"Entropy Loss : $(entropy_l)\")\n",
    "            println(\"Value Loss : $(value_l)\")\n",
    "        end\n",
    "        \n",
    "        if i%SAVE_FREQUENCY == 0\n",
    "        \tif MODE == \"CON\"\n",
    "\t            @save \"weights/policy_mu.bson\" policy_μ\n",
    "\t            @save \"weights/policy_sigma.bson\" policy_Σ\n",
    "\t            @save \"weights/value.bson\" value\n",
    "\t        else\n",
    "\t        \t@save \"weights/policy_cat.bson\" policy\n",
    "\t        \t@save \"weights/value.bson\" value\n",
    "            end\n",
    "            \n",
    "            save(\"stats.jld\",\"rewards\",reward_hist)\n",
    "            println(\"\\n\\n\\n----MAX REWRD SO FAR : $(maximum(reward_hist))---\\n\\n\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP : 1\n",
      "MEAN RETURNS : 5.984589584907436\n",
      "LAST 100 RETURNS : 5.025881888292809\n",
      "Ep done\n",
      "EP : 2\n",
      "MEAN RETURNS : 5.980573360504491\n",
      "LAST 100 RETURNS : 5.025885332573053\n",
      "Ep done\n",
      "EP : 3\n",
      "MEAN RETURNS : 5.976307051952417\n",
      "LAST 100 RETURNS : 5.028993236547197\n",
      "Ep done\n",
      "EP : 4\n",
      "MEAN RETURNS : 5.9711984559149025\n",
      "LAST 100 RETURNS : 5.030545466394145\n",
      "Ep done\n",
      "EP : 5\n",
      "MEAN RETURNS : 5.965854959408277\n",
      "LAST 100 RETURNS : 5.029000099367136\n",
      "Ep done\n",
      "-----___Stats___-----\n",
      "Policy Loss : 0.3179458056853174\n",
      "Entropy Loss : -5.3645862e-11\n",
      "Value Loss : 0.053309712047081764\n",
      "EP : 6\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] ∇broadcast(::getfield(Base.Broadcast, Symbol(\"##2#4\")){getfield(Base.Broadcast, Symbol(\"##8#10\")){getfield(Base.Broadcast, Symbol(\"##1#3\")),getfield(Base.Broadcast, Symbol(\"##5#6\")){getfield(Base.Broadcast, Symbol(\"##5#6\")){getfield(Base.Broadcast, Symbol(\"##7#9\"))}},getfield(Base.Broadcast, Symbol(\"##11#12\")){getfield(Base.Broadcast, Symbol(\"##11#12\")){getfield(Base.Broadcast, Symbol(\"##13#14\"))}},getfield(Base.Broadcast, Symbol(\"##15#16\")){getfield(Base.Broadcast, Symbol(\"##15#16\")){getfield(Base.Broadcast, Symbol(\"##17#18\"))}},typeof(+)},typeof(relu)}, ::TrackedArray{…,Array{Float32,1}}, ::TrackedArray{…,Array{Float32,1}}) at /home/shreyas/.julia/packages/Tracker/6wcYJ/src/lib/array.jl:458",
      " [2] materialize(::Base.Broadcast.Broadcasted{Tracker.TrackedStyle,Nothing,typeof(relu),Tuple{Base.Broadcast.Broadcasted{Tracker.TrackedStyle,Nothing,typeof(+),Tuple{TrackedArray{…,Array{Float32,1}},TrackedArray{…,Array{Float32,1}}}}}}) at /home/shreyas/.julia/packages/Tracker/6wcYJ/src/lib/array.jl:489",
      " [3] Dense at /home/shreyas/.julia/packages/Flux/WSB7k/src/layers/basic.jl:82 [inlined]",
      " [4] Dense at /home/shreyas/.julia/packages/Flux/WSB7k/src/layers/basic.jl:122 [inlined]",
      " [5] (::Dense{typeof(relu),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}})(::Array{Float64,1}) at /home/shreyas/.julia/packages/Flux/WSB7k/src/layers/basic.jl:125",
      " [6] applychain(::Tuple{Dense{typeof(relu),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},Dense{typeof(identity),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},getfield(Main, Symbol(\"##7#8\"))}, ::Array{Float64,1}) at /home/shreyas/.julia/packages/Flux/WSB7k/src/layers/basic.jl:31",
      " [7] (::Chain{Tuple{Dense{typeof(relu),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},Dense{typeof(identity),TrackedArray{…,Array{Float32,2}},TrackedArray{…,Array{Float32,1}}},getfield(Main, Symbol(\"##7#8\"))}})(::Array{Float64,1}) at /home/shreyas/.julia/packages/Flux/WSB7k/src/layers/basic.jl:33",
      " [8] log_prob_from_actions(::Array{Any,1}, ::Array{Any,1}) at ./In[135]:49",
      " [9] process_rollouts(::Array{Array{Any,1},1}) at ./In[136]:56",
      " [10] train_step() at ./In[137]:7",
      " [11] train() at ./In[137]:27",
      " [12] top-level scope at In[138]:1"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy(ones(4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GymEnv CartPole-v0\n",
       "  TimeLimit\n",
       "  r  = 0.0\n",
       "  ∑r = 0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `haskey(o::PyObject, s::Union{Symbol, AbstractString})` is deprecated, use `hasproperty(o, s)` instead.\n",
      "│   caller = show(::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}}, ::GymEnv{PyCall.PyArray{Float64,1}}) at OpenAIGym.jl:64\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:64\n",
      "┌ Warning: `getindex(o::PyObject, s::Symbol)` is deprecated in favor of dot overloading (`getproperty`) so elements should now be accessed as e.g. `o.s` instead of `o[:s]`.\n",
      "│   caller = show(::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}}, ::GymEnv{PyCall.PyArray{Float64,1}}) at OpenAIGym.jl:65\n",
      "└ @ OpenAIGym /home/shreyas/.julia/packages/OpenAIGym/wZkkM/src/OpenAIGym.jl:65\n"
     ]
    }
   ],
   "source": [
    "env = GymEnv(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element PyCall.PyArray{Float64,1}:\n",
       " -0.04849816918922241 \n",
       "  0.013628561471056858\n",
       "  0.006237321324826253\n",
       "  0.04166733079767969 "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = reset!(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(x)\n",
    "    action_probs = policy(x)\n",
    "    mean(action_probs)\n",
    "end\n",
    "\n",
    "gs = Tracker.gradient(() -> loss(s),params(policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 256×4 Array{Float32,2}:\n",
       "  0.0463625    -0.0288894    0.0691652    -0.0266886 \n",
       "  0.124687      0.0818793   -0.129135      0.204254  \n",
       " -0.106862      0.193595    -0.0882367    -0.0241373 \n",
       " -0.0685683     0.0427704    0.0284292     0.0874056 \n",
       " -0.176015      0.104862     0.0349711    -0.124782  \n",
       "  0.0458958    -0.0517615    0.0304131     0.00226418\n",
       "  0.0698005     0.0420365   -0.162521     -0.0820992 \n",
       " -0.0710014    -0.0629289    0.0741035     0.173773  \n",
       "  0.036564      0.0159403    0.0240178    -0.0337437 \n",
       " -0.0883487    -0.074553     0.121145     -0.183758  \n",
       " -0.0945337    -0.00958645  -0.00795938   -0.0662095 \n",
       "  0.0517599    -0.0620799   -0.000125911  -0.0471872 \n",
       " -0.0486801     0.006256     0.178224      0.092455  \n",
       "  ⋮                                                  \n",
       "  0.0060984     0.0074       0.149015      0.0837609 \n",
       " -0.12016      -0.0510779    0.182024      0.0255303 \n",
       " -0.142491     -0.138318    -0.078126     -0.124178  \n",
       "  0.179972      0.124144    -0.0903058    -0.0722095 \n",
       " -0.00565518   -0.0069747   -0.255583      0.053678  \n",
       "  0.148183     -0.08695     -0.0568453    -0.0385866 \n",
       " -0.0178394     0.00701577  -0.0264899    -0.0312972 \n",
       " -0.000947906  -0.0182501   -0.079818      0.114955  \n",
       " -0.0132274    -0.145377     0.107797      0.0581217 \n",
       " -0.0112895    -0.0326152    0.0914327     0.0422812 \n",
       "  0.0581694    -0.154598    -0.106047     -0.179227  \n",
       " -0.0671879     0.203426     0.160432      0.0620347 "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.layers[1].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
