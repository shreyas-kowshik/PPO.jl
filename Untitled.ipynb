{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Rollouts\n",
    "- Define networks\n",
    "- GAE\n",
    "- Utils : Normal distributions and log probability of an action\n",
    "- train\n",
    "- logging utilities\n",
    "\n",
    "Experiment with shared network for both policy and value function\n",
    "\"\"\"\n",
    "\n",
    "using Flux, CuArrays\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using BSON:@save,@load\n",
    "using JLD\n",
    "\n",
    "\"\"\"\n",
    "A few intricacies : \n",
    "The policy is a Normal distribution and the `policy_net` outputs the `μ` and `logσ`.\n",
    "Each action is assumed to be independent of the others.\n",
    "Thus our covariance matrix is a diagonal matrix with each element representing the variance of\n",
    "taking a particular action.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Utilities\n",
    "\"\"\"\n",
    "# weight initialization\n",
    "function _random_normal(shape...)\n",
    "    return map(Float32,rand(Normal(0,0.1),shape...))\n",
    "end\n",
    "\n",
    "function constant_init(shape...)\n",
    "    return map(Float32,ones(shape...) * 0.1)\n",
    "end\n",
    "\n",
    "function normal_log_prob(μ,log_std,a)\n",
    "    \"\"\"\n",
    "    Returns the log probability of an action under a policy Gaussian policy π\n",
    "    \"\"\"\n",
    "    σ = exp.(log_std)\n",
    "    σ² = σ.^2\n",
    "    -(((a .- μ).^2)./(2.0 * σ²)) .- 0.5*log.(sqrt(2 * π)) .- log.(σ)\n",
    "end\n",
    "\n",
    "function normal_entropy(log_std)\n",
    "    0.5 + 0.5 * log(2 * π) .+ log_std\n",
    "end\n",
    "\n",
    "function normalise(arr)\n",
    "    (arr .- mean(arr))./(sqrt(var(arr) + 1e-10))\n",
    "end\n",
    "\n",
    "# Logging #\n",
    "\"\"\"\n",
    "Create logging utility\n",
    "\"\"\"\n",
    "mutable struct Logger\n",
    "    hist_dict\n",
    "end\n",
    "\n",
    "Logger() = Logger(Dict())\n",
    "\n",
    "function register(l::Logger,name::String)\n",
    "     l.hist_dict[name] = []\n",
    "end\n",
    "\n",
    "function add(l::Logger,name,value)\n",
    "    \"\"\"\n",
    "    Add a variable for it's history to be logged\n",
    "    \"\"\"\n",
    "    if !(name in l.hist_dict.keys)\n",
    "        err(\"Error...\")\n",
    "    else\n",
    "        push!(l.hist_dict[name],value)\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "# Policy parameters #\n",
    "η = 3e-4 # Learning rate\n",
    "STD = 0.0 # Standard deviation\n",
    "HIDDEN_SIZE = 256\n",
    "# Environment Variables #\n",
    "STATE_SIZE = 3\n",
    "ACTION_SIZE = 1\n",
    "MIN_RANGE = -2.0f0\n",
    "MAX_RANGE = 2.0f0\n",
    "EPISODE_LENGTH = 20\n",
    "TEST_STEPS = 10000\n",
    "# GAE parameters\n",
    "γ = 0.99\n",
    "λ = 0.95\n",
    "# Optimization parameters\n",
    "PPO_EPOCHS = 4\n",
    "NUM_EPISODES = 2000\n",
    "BATCH_SIZE = 5\n",
    "c₀ = 1.0\n",
    "c₁ = 0.5\n",
    "c₂ = 0.001\n",
    "# PPO parameters\n",
    "ϵ = 0.2\n",
    "# FREQUENCIES\n",
    "SAVE_FREQUENCY = 50\n",
    "VERBOSE_FREQUENCY = 50\n",
    "global_step = 0\n",
    "# policy_type = \"gaussian\"\n",
    "\n",
    "reward_hist = []\n",
    "\n",
    "mutable struct PendulumPolicy <: Reinforce.AbstractPolicy\n",
    "  train::Bool\n",
    "\n",
    "  function PendulumPolicy(train = true)\n",
    "    new(train)\n",
    "  end\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Define the networks\n",
    "\"\"\"\n",
    "# if policy_type == \"gaussian\"\n",
    "policy_μ = Chain(Dense(STATE_SIZE,HIDDEN_SIZE,relu;initW = _random_normal,initb=constant_init),\n",
    "                 Dense(HIDDEN_SIZE,ACTION_SIZE;initW = _random_normal,initb=constant_init),\n",
    "                 x->tanh.(x),\n",
    "                 x->param(2.0) .* x) \n",
    "policy_Σ = param(ones(ACTION_SIZE) * STD)\n",
    "\n",
    "value = Chain(Dense(STATE_SIZE,HIDDEN_SIZE,relu),\n",
    "                  Dense(HIDDEN_SIZE,1))\n",
    "\n",
    "# elseif policy_type == \"linear\"\n",
    "#     policy = Chain(Dense(STATE_SIZE,HIDDEN_SIZE,relu;initW = _random_normal,initb=constant_init),\n",
    "#                      Dense(HIDDEN_SIZE,ACTION_SIZE;initW = _random_normal,initb=constant_init),\n",
    "#                      x->tanh.(x))\n",
    "    \n",
    "#     value = Chain(Dense(STATE_SIZE,HIDDEN_SIZE,relu),\n",
    "#                   Dense(HIDDEN_SIZE,1))\n",
    "# end\n",
    "\n",
    "# Optimizer\n",
    "opt = ADAM(η)\n",
    "\n",
    "function action(state)\n",
    "    # Acccounting for the element type\n",
    "    state = reshape(Array(state),length(state),1)\n",
    "    \n",
    "    a = nothing\n",
    "    # Our policy outputs the parameters of a Normal distribution\n",
    "#     if policy_type == \"gaussian\"\n",
    "    μ = policy_μ(state)\n",
    "    μ = reshape(μ,ACTION_SIZE)\n",
    "    log_std = policy_Σ\n",
    "    \n",
    "    σ² = (exp.(log_std)).^2\n",
    "    Σ = diagm(0=>σ².data)\n",
    "    \n",
    "    dis = MvNormal(μ.data,Σ)\n",
    "    \n",
    "    a = rand(dis,ACTION_SIZE)\n",
    "    \n",
    "#     elseif policy_type == \"linear\"\n",
    "#         out = policy(state)\n",
    "#     end\n",
    "    a\n",
    "end\n",
    "\n",
    "function run_episode(env)\n",
    "    experience = []\n",
    "    \n",
    "    s = reset!(env)\n",
    "    for i in 1:EPISODE_LENGTH\n",
    "        a = action(s)\n",
    "        a = convert.(Float64,a)\n",
    "        a = reshape(a,ACTION_SIZE)\n",
    "        \n",
    "        r,s_ = step!(env,a)\n",
    "        push!(experience,(s,a,r,s_))\n",
    "        s = s_\n",
    "        if env.done\n",
    "           break \n",
    "        end\n",
    "    end\n",
    "    experience\n",
    "end\n",
    "\n",
    "\n",
    "function test_run(env)\n",
    "    ep_r = 0.0\n",
    "    \n",
    "    s = reset!(env)\n",
    "    for i in 1:TEST_STEPS\n",
    "        OpenAIGym.render(env)\n",
    "        a = policy_μ(s).data\n",
    "        a = convert.(Float64,a)\n",
    "        a = reshape(a,ACTION_SIZE)\n",
    "        println(\"Action : $a\")\n",
    "        \n",
    "        r,s_ = step!(env,a)\n",
    "        ep_r += r\n",
    "        \n",
    "        s = s_\n",
    "        if env.done\n",
    "           break \n",
    "        end\n",
    "    end\n",
    "    ep_r\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rollout collection\n",
    "\"\"\"\n",
    "num_processes = 9\n",
    "addprocs(num_processes) \n",
    "\n",
    "@everywhere function collect(env)\n",
    "    run_episode(env)\n",
    "end\n",
    "\n",
    "@everywhere function rollout()\n",
    "  env = GymEnv(:Pendulum,:v0)\n",
    "  return collect(env)\n",
    "end\n",
    "\n",
    "function get_rollouts()\n",
    "    g = []\n",
    "    for  w in workers()\n",
    "      push!(g, rollout())\n",
    "    end\n",
    "\n",
    "    rollouts = fetch.(g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gae(states,actions,rewards,next_states)\n",
    "    \"\"\"\n",
    "    Returns a Generalized Advantage Estimate for an episode\n",
    "    \"\"\"\n",
    "    Â = []\n",
    "    A = 0.0\n",
    "    for i in reverse(1:length(states))\n",
    "        δ = rewards[i] + γ*value(next_states[i]).data[1] - value(states[i]).data[1]\n",
    "        A = δ + (γ*λ*A)\n",
    "        push!(Â,A)\n",
    "    end\n",
    "    \n",
    "    Â = reverse(Â)\n",
    "    return Â\n",
    "end\n",
    "\n",
    "function disconunted_returns(rewards)\n",
    "    r = 0.0\n",
    "    returns = []\n",
    "    for i in reverse(1:length(rewards))\n",
    "        r = rewards[i] + γ*r\n",
    "        push!(returns,r)\n",
    "    end\n",
    "    returns = reverse(returns)\n",
    "    returns\n",
    "end\n",
    "\n",
    "function log_prob_from_actions(states,actions)\n",
    "    \"\"\"\n",
    "    Returns log probabilities of the actions taken\n",
    "    \n",
    "    states,actions : episode vairbles in the form of a list\n",
    "    \"\"\"\n",
    "    log_probs = []\n",
    "    \n",
    "    for i in 1:length(states)\n",
    "        μ = reshape(policy_μ(states[i]),ACTION_SIZE).data\n",
    "        logΣ = policy_Σ.data\n",
    "        push!(log_probs,normal_log_prob(μ,logΣ,actions[i]))\n",
    "    end\n",
    "    \n",
    "    log_probs\n",
    "end\n",
    "\n",
    "function process_rollouts(rollouts)\n",
    "    \"\"\"\n",
    "    rollouts : variable returned by calling `get_rollouts`\n",
    "    \n",
    "    Returns : \n",
    "    states, actions, rewards for minibatch processing\n",
    "    \"\"\"\n",
    "    # Process the variables\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    advantages = []\n",
    "    returns = []\n",
    "    log_probs = []\n",
    "    \n",
    "    # Logging statistics\n",
    "    episode_mean_returns = []\n",
    "    \n",
    "    for ro in rollouts\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        episode_next_states = []\n",
    "        \n",
    "        for i in 1:length(ro)\n",
    "             push!(episode_states,Array(ro[i][1]))\n",
    "             push!(episode_actions,ro[i][2])\n",
    "             push!(episode_rewards,ro[i][3])\n",
    "             push!(episode_next_states,ro[i][4])\n",
    "        end\n",
    "        \n",
    "#         println(\"Ep Max A : $(maximum(episode_actions))\")\n",
    "        \n",
    "        episode_advantages = gae(episode_states,episode_actions,episode_rewards,episode_next_states)\n",
    "        episode_returns = disconunted_returns(episode_rewards)\n",
    "        \n",
    "        push!(episode_mean_returns,mean(episode_returns))\n",
    "        \n",
    "        episode_advantages = normalise(episode_advantages)\n",
    "        \n",
    "        push!(states,episode_states)\n",
    "        push!(actions,episode_actions)\n",
    "        push!(rewards,episode_rewards)\n",
    "        push!(advantages,episode_advantages)\n",
    "        push!(returns,episode_returns)\n",
    "        push!(log_probs,log_prob_from_actions(episode_states,episode_actions))\n",
    "    end\n",
    "    \n",
    "    states = cat(states...,dims=1)\n",
    "    actions = cat(actions...,dims=1)\n",
    "    rewards = cat(rewards...,dims=1)\n",
    "    advantages = cat(advantages...,dims=1)\n",
    "    returns = cat(returns...,dims=1)\n",
    "    log_probs = cat(log_probs...,dims=1)\n",
    "    \n",
    "    push!(reward_hist,mean(episode_mean_returns))\n",
    "    \n",
    "    if length(reward_hist) <= 100\n",
    "        println(\"RETURNS : $(mean(episode_mean_returns))\")\n",
    "    else\n",
    "        println(\"LAST 100 RETURNS : $(mean(reward_hist[end-100:end]))\")\n",
    "    end\n",
    "    \n",
    "    return hcat(states...),hcat(actions...),hcat(rewards...),hcat(advantages...),hcat(returns...),hcat(log_probs...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function print_losses(pl,vl,el) \n",
    "   println(\"------\")\n",
    "   println(\"Policy Loss : $pl\")\n",
    "   println(\"Value Loss : $vl\")\n",
    "   println(\"Entropy Loss : $el\") \n",
    "   println(\"------\")\n",
    "end\n",
    "\n",
    "function loss(states,actions,advantages,returns,old_log_probs)\n",
    "#     println(\"---\")\n",
    "#     println(size(states))\n",
    "#     println(\"States : $states\")\n",
    "#     println(\"---\")\n",
    "#     println(\"Actions : $actions\")\n",
    "#     println(\"---\")\n",
    "    global global_step\n",
    "    global_step += 1\n",
    "    \n",
    "    μ = policy_μ(states)\n",
    "    logΣ = policy_Σ\n",
    "    \n",
    "#     println(\"μ : $μ\")\n",
    "#     println(\"---\")\n",
    "#     println(\"logΣ : $logΣ\")\n",
    "#     println(\"---\")\n",
    "    \n",
    "    new_log_probs = normal_log_prob(μ,logΣ,actions)\n",
    "#     println(\"New Log Probs : $new_log_probs\")\n",
    "#     println(\"Old Log Probs : $old_log_probs\")\n",
    "    \n",
    "    # Surrogate loss computation\n",
    "    ratio = exp.(new_log_probs .- old_log_probs)\n",
    "    surr1 = ratio .* advantages\n",
    "    surr2 = clamp.(ratio,1.0 - ϵ,1.0 + ϵ)\n",
    "    policy_loss = mean(min.(surr1,surr2))\n",
    "    \n",
    "#     println(\"Surr1 : $surr1\")\n",
    "#     println(\"Surr2 : $surr2\")\n",
    "#     println(\"Policy Loss : $policy_loss\")\n",
    "    \n",
    "    value_predicted = value(states)\n",
    "    value_loss = mean((value_predicted .- returns).^2)\n",
    "#     println(\"Value Loss : $value_loss\")\n",
    "    \n",
    "    entropy_loss = mean(normal_entropy(logΣ))\n",
    "    \n",
    "#     if global_step % VERBOSE_FREQUENCY == 0\n",
    "#         print_losses(policy_loss.data,value_loss.data,entropy_loss.data)\n",
    "#     end\n",
    "    \n",
    "    -c₀*policy_loss + c₁*value_loss # - c₂*entropy_loss\n",
    "end\n",
    "\n",
    "function ppo_update(states,actions,advantages,returns,old_log_probs)\n",
    "    # Define model parameters\n",
    "    model_params = params(params(policy_μ)...,params(policy_Σ)...,params(value)...)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gs = Tracker.gradient(() -> loss(states,actions,advantages,returns,old_log_probs),model_params)\n",
    "#     println(\"Gradient Done\")\n",
    "    \n",
    "    g = gs[policy_μ.layers[1].W]\n",
    "#     println(\"GRAD : $(mean(g))\")\n",
    "    # Take a step of optimisation\n",
    "    update!(opt,model_params,gs)\n",
    "#     println(\"Update Done\")\n",
    "end\n",
    "\n",
    "function train_step()    \n",
    "    routs = get_rollouts()\n",
    "    states,actions,rewards,advantages,returns,log_probs = process_rollouts(routs)\n",
    "    \n",
    "    idxs = partition(1:size(states)[end],BATCH_SIZE)\n",
    "    \n",
    "    for epoch in 1:PPO_EPOCHS\n",
    "#         println(\"Epoch : $epoch\")\n",
    "        for i in idxs\n",
    "#             println(i)\n",
    "            mb_states = states[:,i] \n",
    "            mb_actions = actions[:,i] \n",
    "            mb_advantages = advantages[:,i] \n",
    "            mb_returns = returns[:,i] \n",
    "            mb_log_probs = log_probs[:,i]\n",
    "            \n",
    "            ppo_update(mb_states,mb_actions,mb_advantages,mb_returns,mb_log_probs)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function train()\n",
    "    for i in 1:NUM_EPISODES\n",
    "        println(\"EP : $i\")\n",
    "        train_step()\n",
    "        println(\"Ep done\")\n",
    "        \n",
    "        if i%SAVE_FREQUENCY == 0\n",
    "            @save \"weights/policy_mu.bson\" policy_μ\n",
    "            @save \"weights/policy_sigma.bson\" policy_Σ\n",
    "            @save \"weights/value.bson\" value\n",
    "            \n",
    "            save(\"stats.jld\",\"rewards\",reward_hist)\n",
    "            println(\"\\n\\n\\n----MAX REWRD SO FAR : $(maximum(reward_hist))---\\n\\n\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the policy #\n",
    "env = GymEnv(\"Pendulum-v0\")\n",
    "env.pyenv._max_episode_steps = 50000\n",
    "TEST_STEPS = 50000\n",
    "\n",
    "r = test_run(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_base(Array(ro[1][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function loss(a)\n",
    "    r = exp.(a .- (ones(size(a)) |> gpu))\n",
    "    s1 = r .* (rand(size(a)) |> gpu)\n",
    "    s2 = clamp.(a,0.9,1.1)\n",
    "    -1.0 * mean(min.(s1,s2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = Tracker.gradient(() -> loss(out),params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs[m.layers[1].W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Flux,CuArrays\n",
    "using Flux:Tracker\n",
    "using Statistics\n",
    "\n",
    "m = Chain(Conv((3,3),3=>64)) |> gpu\n",
    "x = rand(256,256,3,1) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function loss(x)\n",
    "   out = m(x)\n",
    "   mean(out)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function test()\n",
    "    @time o = loss(x)\n",
    "    @time gs = Tracker.gradient(() -> loss(x),params(m))\n",
    "end\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Flux,CuArrays\n",
    "using Flux:Tracker\n",
    "using Flux:@treelike\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct Net\n",
    "    u\n",
    "end\n",
    "\n",
    "@treelike Net\n",
    "\n",
    "function Net()\n",
    "   Net(Chain(Conv((3,3),3=>64))) \n",
    "end\n",
    "\n",
    "function (n::Net)(x)\n",
    "   return n.u(x) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Net() |> gpu\n",
    "x = rand(256,256,3,1) |> gpu\n",
    "\n",
    "function loss(x)\n",
    "   out = m(x)\n",
    "   mean(out)\n",
    "end\n",
    "\n",
    "function test()\n",
    "    @time o = loss(x)\n",
    "    @time gs = Tracker.gradient(() -> loss(x),params(m))\n",
    "end\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = param(ones(2)) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reward_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean(reward_hist[end-100-100:end-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD\n",
    "save(\"stats.jld\",\"rewards\",reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rh = load(\"stats.jld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"~/envs/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy_Σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "if i == 1\n",
    "    a = 3\n",
    "else\n",
    "    b = 5\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OpenAIGym\n",
    "env = GymEnv(:Pendulum, :v0)\n",
    "for i ∈ 1:20\n",
    "  T = 0\n",
    "  R = run_episode(env, RandomPolicy()) do (s, a, r, s′)\n",
    "    render(env)\n",
    "    T += 1\n",
    "  end\n",
    "  @info(\"Episode $i finished after $T steps. Total reward: $R\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Total Steps : 10000 ::: Total Reward : -344.9227815301468---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test\n",
    "\"\"\"\n",
    "\n",
    "using Flux, CuArrays\n",
    "using OpenAIGym\n",
    "import Reinforce.action\n",
    "import Reinforce:run_episode\n",
    "import Flux.params\n",
    "using Flux.Tracker: grad, update!\n",
    "using Flux: onehot\n",
    "using Statistics\n",
    "using Distributed\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Base.Iterators\n",
    "using BSON:@save,@load\n",
    "using JLD\n",
    "\n",
    "ACTION_SIZE = 1\n",
    "TEST_STEPS = 10000\n",
    "\n",
    "# Load the policy\n",
    "@load \"./weights/pendulum-working/policy_mu.bson\" policy_μ\n",
    "@load \"./weights/pendulum-working/policy_sigma.bson\" policy_Σ\n",
    "\n",
    "# Test Run Function\n",
    "function test_run(env)\n",
    "    ep_r = 0.0\n",
    "    \n",
    "    s = reset!(env)\n",
    "    for i in 1:TEST_STEPS\n",
    "        OpenAIGym.render(env)\n",
    "        a = policy_μ(s).data\n",
    "        a = convert.(Float64,a)\n",
    "        a = reshape(a,ACTION_SIZE)\n",
    "\n",
    "        r,s_ = step!(env,a)\n",
    "        ep_r += r\n",
    "        \n",
    "        s = s_\n",
    "        if env.done\n",
    "           break \n",
    "        end\n",
    "    end\n",
    "    ep_r\n",
    "end\n",
    "\n",
    "env = GymEnv(\"Pendulum-v0\")\n",
    "env.pyenv._max_episode_steps = TEST_STEPS\n",
    "\n",
    "r = test_run(env)\n",
    "println(\"---Total Steps : $TEST_STEPS ::: Total Reward : $r---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
